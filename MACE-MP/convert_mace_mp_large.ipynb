{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.211386024367243\n",
      "27.211386024367243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '' \n",
    "import jax\n",
    "\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "\n",
    "from mace.calculators import mace_mp,mace_off\n",
    "from ase import build\n",
    "from fennol import FENNIX\n",
    "import yaml\n",
    "import jax.numpy as jnp\n",
    "import numpy as np  \n",
    "from fennol.ase import FENNIXCalculator\n",
    "from fennol.utils import AtomicUnits as au\n",
    "from copy import deepcopy\n",
    "from mace import modules as torch_modules\n",
    "import e3nn as e3nn_torch\n",
    "import torch\n",
    "from fennol.utils.activations import normalize_activation\n",
    "\n",
    "from ase.units import Hartree\n",
    "print(au.EV)\n",
    "print(Hartree)\n",
    "\n",
    "e3nn_torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Materials Project MACE for MACECalculator with /home/pthomas/.cache/mace/5f5yavf3\n",
      "Using float64 for MACECalculator, which is slower but more accurate. Recommended for geometry optimization.\n",
      "species_order: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94]\n",
      "94\n",
      "89\n",
      "scale 0.02955895460120917\n",
      "shift 0.006055514143288505\n",
      "avg_num_neighbors 25.57754135131836\n",
      "ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(89x0e -> 256x0e | 22784 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.5, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.5)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-3.6674, -1.3220, -3.4782, -4.7250, -7.7279, -8.4045, -7.3594, -7.2844, -4.8962, -0.0301, -2.7527, -2.8138, -4.8444, -7.6966, -6.9632, -4.6738, -2.8140, -0.0623, -2.6252, -5.3848, -7.9162, -10.2708, -8.6724, -9.2372, -8.3056, -7.0547, -5.5709, -5.1741, -3.2510, -1.2880, -3.5322, -4.7113, -3.9718, -3.8862, -2.5183, 6.7068, -2.5661, -4.9390, -10.1536, -11.8571, -12.1416, -8.7861, -8.7954, -7.7784, -6.8387, -4.8812, -2.0606, -0.6440, -2.7886, -3.8168, -3.5880, -2.8806, -1.6361, 9.7960, -2.7486, -4.9867, -8.9295, -8.7417, -8.0122, -8.2336, -7.6245, -8.1614, -13.5915, -18.5381, -7.6446, -8.1150, -7.6146, -6.8296, -7.8200, -3.5770, -7.4692, -12.7706, -14.1110, -9.3544, -11.3765, -9.6116, -7.3213, -5.3043, -2.3873, 0.2479, -2.3155, -3.7254, -3.4302, -5.0659, -11.0175, -12.2443, -13.8466, -14.9527, -15.3033])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(256x0e -> 256x0e | 65536 weights)\n",
      "      (conv_tp): TensorProduct(256x0e x 1x0e+1x1o+1x2e+1x3o -> 256x0e+256x1o+256x2e+256x3o | 1024 paths | 1024 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 1024]\n",
      "      (linear): Linear(256x0e+256x1o+256x2e+256x3o -> 256x0e+256x1o+256x2e+256x3o | 262144 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(256x0e x 89x0e -> 256x0e+256x1o | 5832704 paths | 5832704 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(256x0e+256x1o -> 256x0e+256x1o | 131072 weights)\n",
      "      (conv_tp): TensorProduct(256x0e+256x1o x 1x0e+1x1o+1x2e+1x3o -> 512x0e+768x1o+768x2e+512x3o | 2560 paths | 2560 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 2560]\n",
      "      (linear): Linear(512x0e+768x1o+768x2e+512x3o -> 256x0e+256x1o+256x2e+256x3o | 655360 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(256x0e+256x1o x 89x0e -> 256x0e | 5832704 paths | 5832704 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 89x4x256]\n",
      "                (1): Parameter containing: [torch.float64 of size 89x1x256]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 89x6x256]\n",
      "                (1): Parameter containing: [torch.float64 of size 89x1x256]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(256x0e+256x1o -> 256x0e+256x1o | 131072 weights)\n",
      "    )\n",
      "    (1): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 89x4x256]\n",
      "                (1): Parameter containing: [torch.float64 of size 89x1x256]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(256x0e -> 256x0e | 65536 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(256x0e+256x1o -> 1x0e | 256 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(256x0e -> 16x0e | 4096 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=0.804340, shift=0.164779)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "atoms = build.molecule('H2O')\n",
    "model_name = \"mace_mp_large\"\n",
    "atoms.set_positions([[0.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0]])\n",
    "calc = mace_mp(model=\"large\", dispersion=False, default_dtype=\"float64\", device='cpu')\n",
    "atoms.calc = calc\n",
    "# print(atoms.get_potential_energy())\n",
    "\n",
    "torch_model_mp = calc.models[0].cpu()\n",
    "species_order = torch_model_mp.atomic_numbers.tolist()\n",
    "print(\"species_order:\",species_order)\n",
    "print(max(species_order))\n",
    "num_species = len(species_order)\n",
    "print(num_species)\n",
    "\n",
    "scale = torch_model_mp.scale_shift.scale.item()\n",
    "shift = torch_model_mp.scale_shift.shift.item()\n",
    "print(\"scale\",scale/au.EV)\n",
    "print(\"shift\",shift/au.EV)\n",
    "\n",
    "avg_num_neigh = torch_model_mp.interactions[0].avg_num_neighbors\n",
    "print(\"avg_num_neighbors\",avg_num_neigh)\n",
    "print(torch_model_mp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6791767923989418\n",
      "r_max=4.5,num_basis=8,p=5.0,max_ell=3,num_inter=2,num_species=89,avg_num_neigh=25.57754135131836,corr=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/mace/modules/models.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"atomic_numbers\", torch.tensor(atomic_numbers, dtype=torch.int64)\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/mace/modules/blocks.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(atomic_energies, dtype=torch.get_default_dtype()),\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(89x0e -> 256x0e | 22784 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.5, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.5)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-3.6674, -1.3220, -3.4782, -4.7250, -7.7279, -8.4045, -7.3594, -7.2844, -4.8962, -0.0301, -2.7527, -2.8138, -4.8444, -7.6966, -6.9632, -4.6738, -2.8140, -0.0623, -2.6252, -5.3848, -7.9162, -10.2708, -8.6724, -9.2372, -8.3056, -7.0547, -5.5709, -5.1741, -3.2510, -1.2880, -3.5322, -4.7113, -3.9718, -3.8862, -2.5183, 6.7068, -2.5661, -4.9390, -10.1536, -11.8571, -12.1416, -8.7861, -8.7954, -7.7784, -6.8387, -4.8812, -2.0606, -0.6440, -2.7886, -3.8168, -3.5880, -2.8806, -1.6361, 9.7960, -2.7486, -4.9867, -8.9295, -8.7417, -8.0122, -8.2336, -7.6245, -8.1614, -13.5915, -18.5381, -7.6446, -8.1150, -7.6146, -6.8296, -7.8200, -3.5770, -7.4692, -12.7706, -14.1110, -9.3544, -11.3765, -9.6116, -7.3213, -5.3043, -2.3873, 0.2479, -2.3155, -3.7254, -3.4302, -5.0659, -11.0175, -12.2443, -13.8466, -14.9527, -15.3033])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(256x0e -> 256x0e | 65536 weights)\n",
      "      (conv_tp): TensorProduct(256x0e x 1x0e+1x1o+1x2e+1x3o -> 256x0e+256x1o+256x2e+256x3o | 1024 paths | 1024 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 1024]\n",
      "      (linear): Linear(256x0e+256x1o+256x2e+256x3o -> 256x0e+256x1o+256x2e+256x3o | 262144 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(256x0e x 89x0e -> 256x0e+256x1o | 5832704 paths | 5832704 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(256x0e+256x1o -> 256x0e+256x1o | 131072 weights)\n",
      "      (conv_tp): TensorProduct(256x0e+256x1o x 1x0e+1x1o+1x2e+1x3o -> 512x0e+768x1o+768x2e+512x3o | 2560 paths | 2560 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 2560]\n",
      "      (linear): Linear(512x0e+768x1o+768x2e+512x3o -> 256x0e+256x1o+256x2e+256x3o | 655360 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(256x0e+256x1o x 89x0e -> 256x0e | 5832704 paths | 5832704 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 89x4x256]\n",
      "                (1): Parameter containing: [torch.float64 of size 89x1x256]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 89x6x256]\n",
      "                (1): Parameter containing: [torch.float64 of size 89x1x256]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(256x0e+256x1o -> 256x0e+256x1o | 131072 weights)\n",
      "    )\n",
      "    (1): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 89x4x256]\n",
      "                (1): Parameter containing: [torch.float64 of size 89x1x256]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(256x0e -> 256x0e | 65536 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(256x0e+256x1o -> 1x0e | 256 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(256x0e -> 16x0e | 4096 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=0.804340, shift=0.164779)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "r_max = torch_model_mp.radial_embedding.bessel_fn.r_max.item()\n",
    "num_basis = torch_model_mp.radial_embedding.bessel_fn.bessel_weights.numel()\n",
    "p = torch_model_mp.radial_embedding.cutoff_fn.p.item()\n",
    "max_ell = torch_model_mp.spherical_harmonics._lmax\n",
    "num_inter = len(torch_model_mp.products)\n",
    "corr = len(torch_model_mp.products[0].symmetric_contractions.contractions[0].weights)+1\n",
    "print(torch_model_mp.readouts[1].non_linearity.acts[0].cst)\n",
    "print(f\"{r_max=},{num_basis=},{p=},{max_ell=},{num_inter=},{num_species=},{avg_num_neigh=},{corr=}\")\n",
    "torch_model = torch_modules.ScaleShiftMACE(\n",
    "    atomic_inter_scale=scale,\n",
    "    atomic_inter_shift=shift,\n",
    "    r_max=r_max,\n",
    "    num_bessel=num_basis,\n",
    "    num_polynomial_cutoff=p,\n",
    "    max_ell=max_ell,\n",
    "    interaction_cls_first=torch_modules.RealAgnosticResidualInteractionBlock,\n",
    "    interaction_cls=torch_modules.RealAgnosticResidualInteractionBlock,\n",
    "    num_interactions=num_inter,\n",
    "    num_elements=num_species,\n",
    "    hidden_irreps=e3nn_torch.o3.Irreps(\"256x0e+256x1o\"),\n",
    "    MLP_irreps=\"16x0e\",\n",
    "    avg_num_neighbors=avg_num_neigh,\n",
    "    correlation=corr,\n",
    "    atomic_energies=torch_model_mp.atomic_energies_fn.atomic_energies.clone().detach(),\n",
    "    atomic_numbers=torch_model_mp.atomic_numbers.clone().detach(),\n",
    "    gate=torch.nn.SiLU(),\n",
    ").double()\n",
    "print(torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Materials Project MACE for MACECalculator with /home/pthomas/.cache/mace/5f5yavf3\n",
      "Using float64 for MACECalculator, which is slower but more accurate. Recommended for geometry optimization.\n",
      "-14.017221861246675\n"
     ]
    }
   ],
   "source": [
    "torch_model.load_state_dict(torch_model_mp.state_dict())\n",
    "calc = mace_mp(model=\"large\", dispersion=False, default_dtype=\"float64\", device='cpu')\n",
    "calc.models[0] = torch_model\n",
    "atoms.calc = calc\n",
    "print(calc.get_potential_energy(atoms,force_consistent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(torch_model.products[0].symmetric_contractions.contractions[0].correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "model_params = yaml.load(open(model_name+\".yaml\"), Loader=yaml.FullLoader)\n",
    "model_fennol = FENNIX(**model_params,rng_key=jax.random.PRNGKey(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymmetricContraction_0\n",
      "    w1_0e\n",
      "    w1_1o\n",
      "    w2_0e\n",
      "    w2_1o\n",
      "    w3_0e\n",
      "    w3_1o\n",
      "SymmetricContraction_1\n",
      "    w1_0e\n",
      "    w2_0e\n",
      "    w3_0e\n",
      "hidden_linear_readout_last\n",
      "    w[0,0] 256x0e,16x0e\n",
      "linear_contraction_0\n",
      "    w[0,0] 256x0e,256x0e\n",
      "    w[1,1] 256x1o,256x1o\n",
      "linear_contraction_1\n",
      "    w[0,0] 256x0e,256x0e\n",
      "linear_dn_0\n",
      "    w[0,0] 256x0e,256x0e\n",
      "    w[1,1] 256x1o,256x1o\n",
      "    w[2,2] 256x2e,256x2e\n",
      "    w[3,3] 256x3o,256x3o\n",
      "linear_dn_1\n",
      "    w[0,0] 512x0e,256x0e\n",
      "    w[1,1] 768x1o,256x1o\n",
      "    w[2,2] 768x2e,256x2e\n",
      "    w[3,3] 512x3o,256x3o\n",
      "linear_readout_0\n",
      "    w[0,0] 256x0e,1x0e\n",
      "linear_readout_last\n",
      "    w[0,0] 16x0e,1x0e\n",
      "linear_up_0\n",
      "    w[0,0] 256x0e,256x0e\n",
      "linear_up_1\n",
      "    w[0,0] 256x0e,256x0e\n",
      "    w[1,1] 256x1o,256x1o\n",
      "radial_network_0\n",
      "    Dense_0\n",
      "    Dense_1\n",
      "    Dense_2\n",
      "    Dense_3\n",
      "radial_network_1\n",
      "    Dense_0\n",
      "    Dense_1\n",
      "    Dense_2\n",
      "    Dense_3\n",
      "skip_tp_0\n",
      "    w[0,0] 256x0e,256x0e\n",
      "skip_tp_1\n",
      "    w[0,0] 256x0e,256x0e\n",
      "species_encoding\n"
     ]
    }
   ],
   "source": [
    "variables = deepcopy(model_fennol.variables)\n",
    "nn_energy_params = variables[\"params\"][\"nn_energy\"]\n",
    "\n",
    "for k in nn_energy_params.keys():\n",
    "    print(k)\n",
    "    if isinstance(nn_energy_params[k],dict):\n",
    "        for kk in nn_energy_params[k].keys():\n",
    "            print(\"   \",kk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_torch_to_jax(linear):\n",
    "    return {\n",
    "        f\"w[{ins.i_in},{ins.i_out}] {linear.irreps_in[ins.i_in]},{linear.irreps_out[ins.i_out]}\": jnp.asarray(\n",
    "            w.data\n",
    "        )\n",
    "        for i, ins, w in linear.weight_views(yield_instruction=True)\n",
    "    }\n",
    "\n",
    "def skip_tp_torch_to_jax(tp):\n",
    "    return {\n",
    "        f\"w[{ins.i_in1},{ins.i_out}] {tp.irreps_in1[ins.i_in1]},{tp.irreps_out[ins.i_out]}\": jnp.moveaxis(\n",
    "            jnp.asarray(w.data), 1, 0\n",
    "        )/num_species**0.5\n",
    "        for i, ins, w in tp.weight_views(yield_instruction=True)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05964225  0.03127083 -0.01582405 -0.03060492  0.0286575  -0.01841103\n",
      " -0.22729727 -0.17191443  0.00620572  0.08738091]\n",
      "[ 0.56266386  0.29500845 -0.14928375 -0.28872619  0.27035428 -0.17368927\n",
      " -2.14431819 -1.62183749  0.05854462  0.82434988]\n",
      "[Instruction(i_in=0, i_out=0, path_shape=(89, 256), path_weight=0.105999788000636)]\n"
     ]
    }
   ],
   "source": [
    "## get initial species encodings\n",
    "species_encodings = np.zeros(nn_energy_params[\"species_encoding\"].shape,dtype=np.float64)\n",
    "species_encodings_torch = torch_model.node_embedding.linear.weight.detach().numpy().reshape(num_species,-1)\n",
    "assert species_encodings.shape[1] == species_encodings_torch.shape[1]\n",
    "factor = num_species**-0.5\n",
    "for i,z in enumerate(species_order):\n",
    "    species_encodings[z,:] = species_encodings_torch[i,:]*factor\n",
    "    # print(f\"{z}: {species_encodings[z]}\")\n",
    "            \n",
    "nn_energy_params[\"species_encoding\"] = jnp.array(species_encodings,dtype=jnp.float64)\n",
    "print(species_encodings[1,:10])\n",
    "print(species_encodings_torch[0,:10])\n",
    "print(torch_model.node_embedding.linear.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "89\n",
      "[ 0.         -0.13477364 -0.04858182 -0.12781999 -0.17363932 -0.28399661\n",
      " -0.30886129 -0.27045406 -0.26769809 -0.17993064 -0.00110461 -0.10116055\n",
      " -0.10340619 -0.17802731 -0.28284597 -0.25589336 -0.17175941 -0.10341201\n",
      " -0.00229076 -0.09647349 -0.19788743 -0.2909149  -0.37744571 -0.31870447\n",
      " -0.3394593  -0.30522592 -0.25925441 -0.20472737 -0.19014643 -0.11947335\n",
      " -0.04733186 -0.12980535 -0.17313813 -0.14596006 -0.14281601 -0.09254634\n",
      "  0.24647191 -0.09430087 -0.18150468 -0.37313806 -0.435739   -0.44619728\n",
      " -0.32288302 -0.32322626 -0.28584915 -0.25131857 -0.1793814  -0.07572414\n",
      " -0.02366622 -0.10247825 -0.1402652  -0.13185597 -0.1058598  -0.06012382\n",
      "  0.3599964  -0.10101055 -0.18325677 -0.32815222 -0.3212518  -0.29444187\n",
      " -0.30257916 -0.28019682 -0.29992475 -0.49947663 -0.68126396 -0.28093213\n",
      " -0.29822258 -0.2798303  -0.25098414 -0.28738069 -0.13145413 -0.27448824\n",
      " -0.46931268 -0.51856921 -0.34376807 -0.41807717 -0.35322028 -0.26905146\n",
      " -0.19492964 -0.0877323   0.00911094 -0.08509437 -0.1369077  -0.12605669\n",
      "  0.          0.          0.          0.          0.         -0.18616868\n",
      " -0.40488674 -0.44996835 -0.50885328 -0.5495024  -0.56238562  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# species energy shift\n",
    "\n",
    "print(len(model_fennol.variables[\"params\"][\"energy_shift\"][\"constant\"]))\n",
    "print(len(torch_model.atomic_energies_fn.atomic_energies))\n",
    "energy_shift = np.zeros(len(model_fennol.variables[\"params\"][\"energy_shift\"][\"constant\"]),dtype=np.float64)\n",
    "for i,z in enumerate(species_order):\n",
    "    # print(f\"{z}: {model_fennol.variables['params']['energy_shift']['constant'][z]} {torch_model.atomic_energies_fn.atomic_energies[i].data/au.EV}\")\n",
    "    energy_shift[z] = torch_model.atomic_energies_fn.atomic_energies[i].data/au.EV\n",
    "variables[\"params\"][\"energy_shift\"][\"constant\"] = jnp.array(energy_shift,dtype=jnp.float64)\n",
    "print(variables[\"params\"][\"energy_shift\"][\"constant\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip_tp_0\n",
      "dict_keys(['w[0,0] 256x0e,256x0e'])\n",
      "dict_keys(['w[0,0] 256x0e,256x0e'])\n",
      "(89, 256, 256)\n",
      "(120, 256, 256)\n",
      "skip_tp_1\n",
      "dict_keys(['w[0,0] 256x0e,256x0e'])\n",
      "dict_keys(['w[0,0] 256x0e,256x0e'])\n",
      "(89, 256, 256)\n",
      "(120, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# skip tp 0\n",
    "for layer in range(2):\n",
    "    print(f'skip_tp_{layer}')\n",
    "    skip_params_torch = skip_tp_torch_to_jax(\n",
    "        torch_model.interactions[layer].skip_tp\n",
    "    )\n",
    "    print(skip_params_torch.keys())\n",
    "    print(nn_energy_params[f\"skip_tp_{layer}\"].keys())\n",
    "    for k in nn_energy_params[f'skip_tp_{layer}'].keys():\n",
    "        print(skip_params_torch[k].shape)\n",
    "        print(nn_energy_params[f'skip_tp_{layer}'][k].shape)\n",
    "        skip_params = np.zeros_like(nn_energy_params[f'skip_tp_{layer}'][k])\n",
    "        assert skip_params.shape[1:] == nn_energy_params[f'skip_tp_{layer}'][k].shape[1:]\n",
    "        for i,z in enumerate(species_order):\n",
    "            skip_params[z,:,:] = np.array(skip_params_torch[k][i,:,:])\n",
    "\n",
    "        nn_energy_params[f'skip_tp_{layer}'][k] = jnp.array(skip_params,dtype=jnp.float64)\n",
    "\n",
    "# # skip tp 1\n",
    "# print()\n",
    "# print(\"skip tp 1\")\n",
    "# skip_params_torch = skip_tp_torch_to_jax(\n",
    "#         torch_model.interactions[1].skip_tp\n",
    "#     )\n",
    "# print(skip_params_torch.keys())\n",
    "# print(nn_energy_params[\"skip_tp_1\"].keys())\n",
    "\n",
    "# print(skip_params_torch['w[0,0] 128x0e,128x0e'].shape)\n",
    "# print(nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'].shape)\n",
    "# skip_params = np.zeros_like(nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'])\n",
    "# assert skip_params.shape[1:] == nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'].shape[1:]\n",
    "# for i,z in enumerate(species_order):\n",
    "#     skip_params[z,:,:] = np.array(skip_params_torch['w[0,0] 128x0e,128x0e'][i,:,:])\n",
    "\n",
    "# nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'] = jnp.array(skip_params,dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "linear_up_0\n",
      "dict_keys(['w[0,0] 256x0e,256x0e'])\n",
      "dict_keys(['w[0,0] 256x0e,256x0e'])\n",
      "w[0,0] 256x0e,256x0e\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "\n",
      "linear_up_1\n",
      "dict_keys(['w[0,0] 256x0e,256x0e', 'w[1,1] 256x1o,256x1o'])\n",
      "dict_keys(['w[0,0] 256x0e,256x0e', 'w[1,1] 256x1o,256x1o'])\n",
      "w[0,0] 256x0e,256x0e\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "w[1,1] 256x1o,256x1o\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "\n",
      "linear_dn_0\n",
      "dict_keys(['w[0,0] 256x0e,256x0e', 'w[1,1] 256x1o,256x1o', 'w[2,2] 256x2e,256x2e', 'w[3,3] 256x3o,256x3o'])\n",
      "dict_keys(['w[0,0] 256x0e,256x0e', 'w[1,1] 256x1o,256x1o', 'w[2,2] 256x2e,256x2e', 'w[3,3] 256x3o,256x3o'])\n",
      "w[0,0] 256x0e,256x0e\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "w[1,1] 256x1o,256x1o\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "w[2,2] 256x2e,256x2e\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "w[3,3] 256x3o,256x3o\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "\n",
      "linear_dn_1\n",
      "dict_keys(['w[0,0] 512x0e,256x0e', 'w[1,1] 768x1o,256x1o', 'w[2,2] 768x2e,256x2e', 'w[3,3] 512x3o,256x3o'])\n",
      "dict_keys(['w[0,0] 512x0e,256x0e', 'w[1,1] 768x1o,256x1o', 'w[2,2] 768x2e,256x2e', 'w[3,3] 512x3o,256x3o'])\n",
      "w[0,0] 512x0e,256x0e\n",
      "(512, 256)\n",
      "(512, 256)\n",
      "w[1,1] 768x1o,256x1o\n",
      "(768, 256)\n",
      "(768, 256)\n",
      "w[2,2] 768x2e,256x2e\n",
      "(768, 256)\n",
      "(768, 256)\n",
      "w[3,3] 512x3o,256x3o\n",
      "(512, 256)\n",
      "(512, 256)\n",
      "\n",
      "linear_contraction_0\n",
      "dict_keys(['w[0,0] 256x0e,256x0e', 'w[1,1] 256x1o,256x1o'])\n",
      "dict_keys(['w[0,0] 256x0e,256x0e', 'w[1,1] 256x1o,256x1o'])\n",
      "w[0,0] 256x0e,256x0e\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "w[1,1] 256x1o,256x1o\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "\n",
      "linear_contraction_1\n",
      "dict_keys(['w[0,0] 256x0e,256x0e'])\n",
      "dict_keys(['w[0,0] 256x0e,256x0e'])\n",
      "w[0,0] 256x0e,256x0e\n",
      "(256, 256)\n",
      "(256, 256)\n"
     ]
    }
   ],
   "source": [
    "# linear up\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print(f\"linear_up_{i}\")\n",
    "    up_params_torch = linear_torch_to_jax(torch_model.interactions[i].linear_up)\n",
    "    print(up_params_torch.keys())\n",
    "    print(nn_energy_params[f\"linear_up_{i}\"].keys())\n",
    "    for k in up_params_torch.keys():\n",
    "        print(k)\n",
    "        print(up_params_torch[k].shape)\n",
    "        print(nn_energy_params[f\"linear_up_{i}\"][k].shape)\n",
    "        assert up_params_torch[k].shape == nn_energy_params[f\"linear_up_{i}\"][k].shape\n",
    "        nn_energy_params[f\"linear_up_{i}\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "# linear down\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print(f\"linear_dn_{i}\")\n",
    "    up_params_torch = linear_torch_to_jax(torch_model.interactions[i].linear)\n",
    "    print(up_params_torch.keys())\n",
    "    print(nn_energy_params[f\"linear_dn_{i}\"].keys())\n",
    "    for k in up_params_torch.keys():\n",
    "        print(k)\n",
    "        print(up_params_torch[k].shape)\n",
    "        print(nn_energy_params[f\"linear_dn_{i}\"][k].shape)\n",
    "        assert up_params_torch[k].shape == nn_energy_params[f\"linear_dn_{i}\"][k].shape\n",
    "        nn_energy_params[f\"linear_dn_{i}\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "# contraction linear\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print(f\"linear_contraction_{i}\")\n",
    "    up_params_torch = linear_torch_to_jax(torch_model.products[i].linear)\n",
    "    print(up_params_torch.keys())\n",
    "    print(nn_energy_params[f\"linear_contraction_{i}\"].keys())\n",
    "    for k in up_params_torch.keys():\n",
    "        print(k)\n",
    "        print(up_params_torch[k].shape)\n",
    "        try:\n",
    "            print(nn_energy_params[f\"linear_contraction_{i}\"][k].shape)\n",
    "            assert up_params_torch[k].shape == nn_energy_params[f\"linear_contraction_{i}\"][k].shape\n",
    "            nn_energy_params[f\"linear_contraction_{i}\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "        except KeyError:\n",
    "            print(f\"key {k} not found in nn_energy_params[f'linear_contraction_{i}']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_readout_0\n",
      "dict_keys(['w[0,0] 256x0e,1x0e'])\n",
      "dict_keys(['w[0,0] 256x0e,1x0e'])\n",
      "w[0,0] 256x0e,1x0e\n",
      "(256, 1)\n",
      "(256, 1)\n",
      "\n",
      "hidden_linear_readout_last\n",
      "dict_keys(['w[0,0] 256x0e,16x0e'])\n",
      "dict_keys(['w[0,0] 256x0e,16x0e'])\n",
      "w[0,0] 256x0e,16x0e\n",
      "(256, 16)\n",
      "(256, 16)\n",
      "silu_norm 1.6765620560798837 1.6765647703084872\n",
      "\n",
      "linear_readout_last\n",
      "dict_keys(['w[0,0] 16x0e,1x0e'])\n",
      "dict_keys(['w[0,0] 16x0e,1x0e'])\n",
      "w[0,0] 16x0e,1x0e\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "1.6791767923989418\n"
     ]
    }
   ],
   "source": [
    "# readout block\n",
    "print(f\"linear_readout_0\")\n",
    "up_params_torch = linear_torch_to_jax(torch_model.readouts[0].linear)\n",
    "print(up_params_torch.keys())\n",
    "print(nn_energy_params[f\"linear_readout_0\"].keys())\n",
    "for k in up_params_torch.keys():\n",
    "    print(k)\n",
    "    print(up_params_torch[k].shape)\n",
    "    print(nn_energy_params[f\"linear_readout_0\"][k].shape)\n",
    "    assert up_params_torch[k].shape == nn_energy_params[f\"linear_readout_0\"][k].shape\n",
    "    nn_energy_params[f\"linear_readout_0\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "print()\n",
    "print(f\"hidden_linear_readout_last\")\n",
    "up_params_torch = linear_torch_to_jax(torch_model.readouts[1].linear_1)\n",
    "print(up_params_torch.keys())\n",
    "print(nn_energy_params[f\"hidden_linear_readout_last\"].keys())\n",
    "for k in up_params_torch.keys():\n",
    "    print(k)\n",
    "    print(up_params_torch[k].shape)\n",
    "    print(nn_energy_params[f\"hidden_linear_readout_last\"][k].shape)\n",
    "    assert up_params_torch[k].shape == nn_energy_params[f\"hidden_linear_readout_last\"][k].shape\n",
    "    nn_energy_params[f\"hidden_linear_readout_last\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "\n",
    "f,silu_norm = normalize_activation(jax.nn.silu,return_scale=True)\n",
    "print(\"silu_norm\",silu_norm,1.6765647703084872)\n",
    "cst = torch_model.readouts[1].non_linearity.acts[0].cst / silu_norm\n",
    "\n",
    "print()\n",
    "print(f\"linear_readout_last\")\n",
    "up_params_torch = linear_torch_to_jax(torch_model.readouts[1].linear_2)\n",
    "print(up_params_torch.keys())\n",
    "print(nn_energy_params[f\"linear_readout_last\"].keys())\n",
    "for k in up_params_torch.keys():\n",
    "    print(k)\n",
    "    print(up_params_torch[k].shape)\n",
    "    print(nn_energy_params[f\"linear_readout_last\"][k].shape)\n",
    "    assert up_params_torch[k].shape == nn_energy_params[f\"linear_readout_last\"][k].shape\n",
    "    nn_energy_params[f\"linear_readout_last\"][k] = cst*jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "print(torch_model.readouts[1].non_linearity.acts[0].cst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radial net 0\n",
      "dict_keys(['Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])\n",
      "(8, 64) (8, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 1024) (64, 1024)\n",
      "\n",
      "radial net 1\n",
      "dict_keys(['Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])\n",
      "(8, 64) (8, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 2560) (64, 2560)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# radial net\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"radial net\",i)\n",
    "    print(nn_energy_params[f\"radial_network_{i}\"].keys())\n",
    "    w  = torch_model.interactions[i].conv_tp_weights.layer0.weight.detach().numpy()\n",
    "    w = w/w.shape[0]**0.5\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_0\"][\"kernel\"].shape)\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_0\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_0\"][\"kernel\"] = jnp.array(w,dtype=jnp.float64)\n",
    "\n",
    "    cst = torch_model.interactions[i].conv_tp_weights.layer0.act.cst/ silu_norm\n",
    "    print(\"cst\",cst)\n",
    "\n",
    "    w  = cst*torch_model.interactions[i].conv_tp_weights.layer1.weight.detach().numpy()\n",
    "    w = w/w.shape[0]**0.5\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_1\"][\"kernel\"].shape)\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_1\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_1\"][\"kernel\"] =  jnp.array(w,dtype=jnp.float64)\n",
    "\n",
    "    cst = torch_model.interactions[i].conv_tp_weights.layer0.act.cst/ silu_norm\n",
    "    print(\"cst\",cst)\n",
    "\n",
    "    w  = cst*torch_model.interactions[i].conv_tp_weights.layer2.weight.detach().numpy()\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_2\"][\"kernel\"].shape)\n",
    "    w = w/w.shape[0]**0.5\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_2\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_2\"][\"kernel\"] =  jnp.array(w,dtype=jnp.float64)\n",
    "\n",
    "    cst = torch_model.interactions[i].conv_tp_weights.layer0.act.cst/ silu_norm\n",
    "    print(\"cst\",cst)\n",
    "\n",
    "    w  = cst*torch_model.interactions[i].conv_tp_weights.layer3.weight.detach().numpy()\n",
    "    w = w/w.shape[0]**0.5\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_3\"][\"kernel\"].shape)\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_3\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_3\"][\"kernel\"] =  jnp.array(w,dtype=jnp.float64)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mace.tools.cg import U_matrix_real\n",
    "from e3nn_jax import reduced_tensor_product_basis\n",
    "from e3nn import o3\n",
    "import e3nn_jax\n",
    "\n",
    "def get_reordering(order,ir_out):\n",
    "    # torch \n",
    "    irreps_in = o3.Irreps(\"1x0e + 1x1o + 1x2e + 1x3o\")\n",
    "    irrep_out = o3.Irreps(\"1x\"+ir_out)\n",
    "    U_matrix = U_matrix_real(irreps_in=irreps_in, irreps_out=irrep_out, correlation=order)[-1].numpy()\n",
    "    U_matrix_abs = np.abs(U_matrix)\n",
    "\n",
    "    # jax\n",
    "    irreps_in = e3nn_jax.Irreps(\"1x0e + 1x1o + 1x2e + 1x3o\")\n",
    "    irrep_out = e3nn_jax.Irrep(ir_out)\n",
    "    U = reduced_tensor_product_basis([irreps_in] * order, keep_ir=[irrep_out]).list[-1]\n",
    "    U = np.moveaxis(U,-1,0)\n",
    "    U_abs = np.abs(U)\n",
    "\n",
    "    index = []\n",
    "    signs = []\n",
    "    for i in range(U.shape[-1]):\n",
    "        diffs = []\n",
    "        for k in range(U_matrix.shape[-1]):\n",
    "            diff = np.max(np.abs(U_abs[...,i]-U_matrix_abs[...,k]))\n",
    "            diffs.append(diff)\n",
    "        k = np.argmin(diffs)\n",
    "        diff = diffs[k]\n",
    "        if diff>1.e-6:\n",
    "            print(i,k,diff)\n",
    "            raise ValueError(\"diff too large\")\n",
    "        \n",
    "        index.append(k)\n",
    "        diff = np.max(np.abs(U[...,i]-U_matrix[...,k]))\n",
    "        if diff < 1.e-6:\n",
    "            signs.append(1.)\n",
    "        else:\n",
    "            diff = np.max(np.abs(U[...,i]+U_matrix[...,k]))\n",
    "            if diff < 1.e-6:\n",
    "                print(i,k,\"global sign differ\")\n",
    "                signs.append(-1.)\n",
    "            else:\n",
    "                raise ValueError(\"local sign differ\")\n",
    "    print(order,ir_out,len(index),U.shape[-1],len(signs))\n",
    "    return np.array(index),np.array(signs)\n",
    "    return np.arange(U.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layer 0 -> ['w1_0e', 'w1_1o', 'w2_0e', 'w2_1o', 'w3_0e', 'w3_1o']\n",
      "   ir 0e l 0 corr 3\n",
      "     w3_0e (89, 23, 256) (120, 23, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0e 23 23 23\n",
      "0.023364469645692523\n",
      "     w2_0e (89, 4, 256) (120, 4, 256)\n",
      "2 0e 4 4 4\n",
      "0.06560130721980657\n",
      "     w1_0e (89, 1, 256) (120, 1, 256)\n",
      "1 0e 1 1 1\n",
      "0.12352341570609536\n",
      "   ir 1o l 1 corr 3\n",
      "     w3_1o (89, 51, 256) (120, 51, 256)\n",
      "17 17 global sign differ\n",
      "18 18 global sign differ\n",
      "35 21 global sign differ\n",
      "36 22 global sign differ\n",
      "44 44 global sign differ\n",
      "46 46 global sign differ\n",
      "3 1o 51 51 51\n",
      "0.012337342339413158\n",
      "     w2_1o (89, 6, 256) (120, 6, 256)\n",
      "2 1o 6 6 6\n",
      "0.024140118511619114\n",
      "     w1_1o (89, 1, 256) (120, 1, 256)\n",
      "1 1o 1 1 1\n",
      "0.05139102941315899\n",
      "\n",
      "layer 1 -> ['w1_0e', 'w2_0e', 'w3_0e']\n",
      "   ir 0e l 0 corr 3\n",
      "     w3_0e (89, 23, 256) (120, 23, 256)\n",
      "3 0e 23 23 23\n",
      "0.001676299679452954\n",
      "     w2_0e (89, 4, 256) (120, 4, 256)\n",
      "2 0e 4 4 4\n",
      "0.011771464396225765\n",
      "     w1_0e (89, 1, 256) (120, 1, 256)\n",
      "1 0e 1 1 1\n",
      "0.05506005648868481\n"
     ]
    }
   ],
   "source": [
    "ir_layers = [[\"0e\",\"1o\"],[\"0e\"]]\n",
    "for layer in [0,1]:\n",
    "    print()\n",
    "    print(\"layer\",layer,\"->\",list(nn_energy_params[f\"SymmetricContraction_{layer}\"].keys()))\n",
    "\n",
    "    for ir in ir_layers[layer]:\n",
    "        l=int(ir[0])\n",
    "        corr = len(torch_model.products[layer].symmetric_contractions.contractions[l].weights)+1\n",
    "        print(\"   ir\",ir,\"l\",l,\"corr\",corr)\n",
    "\n",
    "        w=torch_model.products[layer].symmetric_contractions.contractions[l].weights_max.detach().cpu().numpy()\n",
    "\n",
    "        print(f\"     w{corr}_{ir}\",w.shape,nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"].shape) \n",
    "        assert w.shape[1:] == nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"].shape[1:]\n",
    "\n",
    "        # reorder weights\n",
    "        index,signs = get_reordering(corr,ir)\n",
    "        w = w[:,index,:]*signs[None,:,None]\n",
    "\n",
    "\n",
    "        # last order\n",
    "        skip_params = np.zeros_like(nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"])\n",
    "        for i,z in enumerate(species_order):\n",
    "            skip_params[z,:,:] = w[i,:,:]\n",
    "        nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"] = jnp.asarray(skip_params,dtype=jnp.float64)\n",
    "\n",
    "        print(np.abs(w).mean())\n",
    "\n",
    "        for c in range(corr-1):\n",
    "            order = corr-c-1\n",
    "            w=torch_model.products[layer].symmetric_contractions.contractions[l].weights[c].detach().cpu().numpy()\n",
    "            print(f\"     w{order}_{ir}\",w.shape,nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"].shape)\n",
    "            assert w.shape[1:] == nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"].shape[1:]\n",
    "\n",
    "            index,signs = get_reordering(order,ir)\n",
    "            w = w[:,index,:]*signs[None,:,None]\n",
    "\n",
    "            skip_params = np.zeros_like(nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"])\n",
    "            for i,z in enumerate(species_order):\n",
    "                skip_params[z,:,:] = w[i,:,:]\n",
    "            nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"] = jnp.asarray(skip_params,dtype=jnp.float64)\n",
    "            print(np.abs(w).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACE fennol -14.017221861330029\n",
      "MACE torch -14.017221861246675\n"
     ]
    }
   ],
   "source": [
    "variables[\"params\"][\"nn_energy\"] = nn_energy_params  \n",
    "model_fennol.variables = variables \n",
    "\n",
    "model_fennol.save(model_name+\".fnx\")\n",
    "model_fennol_load = FENNIX.load(model_name+\".fnx\")\n",
    "# print(\"##############################\")\n",
    "# print(\"Restored model\")\n",
    "# for k in model_fennol_load.variables[\"params\"][\"nn_energy\"].keys():\n",
    "#     print(k)\n",
    "#     if isinstance(nn_energy_params[k],dict):\n",
    "#         for kk in nn_energy_params[k].keys():\n",
    "#             print(\"   \",kk)\n",
    "calc_fennol = FENNIXCalculator(model=model_fennol_load,atoms=atoms,use_float64=True)\n",
    "atoms.calc = calc_fennol\n",
    "print(\"MACE fennol\",atoms.get_potential_energy())\n",
    "atoms.calc = calc  \n",
    "print(\"MACE torch\",atoms.get_potential_energy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['batch_index', 'coordinates', 'energy_shift', 'graph', 'natoms', 'nn_energy', 'nn_energy_node_feats', 'species'])\n",
      "[-0.02513665  0.31355303  0.31355303]\n"
     ]
    }
   ],
   "source": [
    "species = atoms.get_atomic_numbers()\n",
    "coordinates = atoms.get_positions()\n",
    "output = model_fennol_load(coordinates=coordinates,species=species)\n",
    "print(output.keys())\n",
    "print(output[\"nn_energy\"]*au.EV)\n",
    "# print(output[\"nn_energy_node_feats\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02513665  0.31355303  0.31355303]\n"
     ]
    }
   ],
   "source": [
    "node_feats_torch = calc.get_descriptors(atoms)\n",
    "# print(node_feats_torch[0])\n",
    "print(calc.get_property(\"node_energy\",atoms=atoms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
