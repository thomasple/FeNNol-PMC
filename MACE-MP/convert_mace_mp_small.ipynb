{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.211386024367243\n",
      "27.211386024367243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '' \n",
    "import jax\n",
    "\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "\n",
    "from mace.calculators import mace_mp,mace_off\n",
    "from ase import build\n",
    "from fennol import FENNIX\n",
    "import yaml\n",
    "import jax.numpy as jnp\n",
    "import numpy as np  \n",
    "from fennol.ase import FENNIXCalculator\n",
    "from fennol.utils import AtomicUnits as au\n",
    "from copy import deepcopy\n",
    "from mace import modules as torch_modules\n",
    "import e3nn as e3nn_torch\n",
    "import torch\n",
    "from fennol.utils.activations import normalize_activation\n",
    "\n",
    "from ase.units import Hartree\n",
    "print(au.EV)\n",
    "print(Hartree)\n",
    "\n",
    "e3nn_torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Materials Project MACE for MACECalculator with /home/pthomas/.cache/mace/46jrkm3v\n",
      "Using float64 for MACECalculator, which is slower but more accurate. Recommended for geometry optimization.\n",
      "species_order: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94]\n",
      "94\n",
      "89\n",
      "scale 0.029552110088317672\n",
      "shift 0.006030452232198983\n",
      "avg_num_neighbors 61.964672446250916\n",
      "ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(89x0e -> 128x0e | 11392 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=6.0, num_basis=10, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=6.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-3.6672, -1.3321, -3.4821, -4.7367, -7.7249, -8.4056, -7.3601, -7.2846, -4.8965, 0.0000, -2.7594, -2.8140, -4.8469, -7.6948, -6.9633, -4.6726, -2.8117, -0.0626, -2.6176, -5.3905, -7.8858, -10.2684, -8.6651, -9.2331, -8.3050, -7.0490, -5.5774, -5.1727, -3.2521, -1.2902, -3.5271, -4.7085, -3.9765, -3.8862, -2.5185, 6.7669, -2.5635, -4.9380, -10.1498, -11.8469, -12.1389, -8.7917, -8.7869, -7.7809, -6.8500, -4.8910, -2.0634, -0.6396, -2.7887, -3.8186, -3.5871, -2.8804, -1.6356, 9.8467, -2.7653, -4.9910, -8.9337, -8.7356, -8.0190, -8.2515, -7.5917, -8.1697, -13.5927, -18.5175, -7.6474, -8.1230, -7.6078, -6.8503, -7.8269, -3.5848, -7.4554, -12.7963, -14.1081, -9.3549, -11.3875, -9.6219, -7.3244, -5.3047, -2.3801, 0.2495, -2.3240, -3.7300, -3.4388, -5.0629, -11.0246, -12.2656, -13.8556, -14.9331, -15.2828])\n",
      "  (interactions): ModuleList(\n",
      "    (0-1): 2 x RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(128x0e -> 128x0e | 16384 weights)\n",
      "      (conv_tp): TensorProduct(128x0e x 1x0e+1x1o+1x2e+1x3o -> 128x0e+128x1o+128x2e+128x3o | 512 paths | 512 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[10, 64, 64, 64, 512]\n",
      "      (linear): Linear(128x0e+128x1o+128x2e+128x3o -> 128x0e+128x1o+128x2e+128x3o | 65536 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(128x0e x 89x0e -> 128x0e | 1458176 paths | 1458176 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 89x4x128]\n",
      "                (1): Parameter containing: [torch.float64 of size 89x1x128]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(128x0e -> 128x0e | 16384 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(128x0e -> 1x0e | 128 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(128x0e -> 16x0e | 2048 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=0.804154, shift=0.164097)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "atoms = build.molecule('H2O')\n",
    "model_name = \"mace_mp_small\"\n",
    "atoms.set_positions([[0.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0]])\n",
    "calc = mace_mp(model=\"small\", dispersion=False, default_dtype=\"float64\", device='cpu')\n",
    "atoms.calc = calc\n",
    "# print(atoms.get_potential_energy())\n",
    "\n",
    "torch_model_mp = calc.models[0].cpu()\n",
    "species_order = torch_model_mp.atomic_numbers.tolist()\n",
    "print(\"species_order:\",species_order)\n",
    "print(max(species_order))\n",
    "num_species = len(species_order)\n",
    "print(num_species)\n",
    "\n",
    "scale = torch_model_mp.scale_shift.scale.item()\n",
    "shift = torch_model_mp.scale_shift.shift.item()\n",
    "print(\"scale\",scale/au.EV)\n",
    "print(\"shift\",shift/au.EV)\n",
    "\n",
    "avg_num_neigh = torch_model_mp.interactions[0].avg_num_neighbors\n",
    "print(\"avg_num_neighbors\",avg_num_neigh)\n",
    "print(torch_model_mp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6791767923989418\n",
      "r_max=6.0,num_basis=10,p=5.0,max_ell=3,num_inter=2,num_species=89,avg_num_neigh=61.964672446250916,corr=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/mace/modules/models.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"atomic_numbers\", torch.tensor(atomic_numbers, dtype=torch.int64)\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/mace/modules/blocks.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(atomic_energies, dtype=torch.get_default_dtype()),\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(89x0e -> 128x0e | 11392 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=6.0, num_basis=10, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=6.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-3.6672, -1.3321, -3.4821, -4.7367, -7.7249, -8.4056, -7.3601, -7.2846, -4.8965, 0.0000, -2.7594, -2.8140, -4.8469, -7.6948, -6.9633, -4.6726, -2.8117, -0.0626, -2.6176, -5.3905, -7.8858, -10.2684, -8.6651, -9.2331, -8.3050, -7.0490, -5.5774, -5.1727, -3.2521, -1.2902, -3.5271, -4.7085, -3.9765, -3.8862, -2.5185, 6.7669, -2.5635, -4.9380, -10.1498, -11.8469, -12.1389, -8.7917, -8.7869, -7.7809, -6.8500, -4.8910, -2.0634, -0.6396, -2.7887, -3.8186, -3.5871, -2.8804, -1.6356, 9.8467, -2.7653, -4.9910, -8.9337, -8.7356, -8.0190, -8.2515, -7.5917, -8.1697, -13.5927, -18.5175, -7.6474, -8.1230, -7.6078, -6.8503, -7.8269, -3.5848, -7.4554, -12.7963, -14.1081, -9.3549, -11.3875, -9.6219, -7.3244, -5.3047, -2.3801, 0.2495, -2.3240, -3.7300, -3.4388, -5.0629, -11.0246, -12.2656, -13.8556, -14.9331, -15.2828])\n",
      "  (interactions): ModuleList(\n",
      "    (0-1): 2 x RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(128x0e -> 128x0e | 16384 weights)\n",
      "      (conv_tp): TensorProduct(128x0e x 1x0e+1x1o+1x2e+1x3o -> 128x0e+128x1o+128x2e+128x3o | 512 paths | 512 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[10, 64, 64, 64, 512]\n",
      "      (linear): Linear(128x0e+128x1o+128x2e+128x3o -> 128x0e+128x1o+128x2e+128x3o | 65536 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(128x0e x 89x0e -> 128x0e | 1458176 paths | 1458176 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 89x4x128]\n",
      "                (1): Parameter containing: [torch.float64 of size 89x1x128]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(128x0e -> 128x0e | 16384 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(128x0e -> 1x0e | 128 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(128x0e -> 16x0e | 2048 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=0.804154, shift=0.164097)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "r_max = torch_model_mp.radial_embedding.bessel_fn.r_max.item()\n",
    "num_basis = torch_model_mp.radial_embedding.bessel_fn.bessel_weights.numel()\n",
    "p = torch_model_mp.radial_embedding.cutoff_fn.p.item()\n",
    "max_ell = torch_model_mp.spherical_harmonics._lmax\n",
    "num_inter = len(torch_model_mp.products)\n",
    "corr = len(torch_model_mp.products[0].symmetric_contractions.contractions[0].weights)+1\n",
    "print(torch_model_mp.readouts[1].non_linearity.acts[0].cst)\n",
    "print(f\"{r_max=},{num_basis=},{p=},{max_ell=},{num_inter=},{num_species=},{avg_num_neigh=},{corr=}\")\n",
    "torch_model = torch_modules.ScaleShiftMACE(\n",
    "    atomic_inter_scale=scale,\n",
    "    atomic_inter_shift=shift,\n",
    "    r_max=r_max,\n",
    "    num_bessel=num_basis,\n",
    "    num_polynomial_cutoff=p,\n",
    "    max_ell=max_ell,\n",
    "    interaction_cls_first=torch_modules.RealAgnosticResidualInteractionBlock,\n",
    "    interaction_cls=torch_modules.RealAgnosticResidualInteractionBlock,\n",
    "    num_interactions=num_inter,\n",
    "    num_elements=num_species,\n",
    "    hidden_irreps=e3nn_torch.o3.Irreps(\"128x0e\"),\n",
    "    MLP_irreps=\"16x0e\",\n",
    "    avg_num_neighbors=avg_num_neigh,\n",
    "    correlation=corr,\n",
    "    atomic_energies=torch_model_mp.atomic_energies_fn.atomic_energies.clone().detach(),\n",
    "    atomic_numbers=torch_model_mp.atomic_numbers.clone().detach(),\n",
    "    gate=torch.nn.SiLU(),\n",
    ").double()\n",
    "print(torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Materials Project MACE for MACECalculator with /home/pthomas/.cache/mace/46jrkm3v\n",
      "Using float64 for MACECalculator, which is slower but more accurate. Recommended for geometry optimization.\n",
      "-13.883318032136243\n"
     ]
    }
   ],
   "source": [
    "torch_model.load_state_dict(torch_model_mp.state_dict())\n",
    "calc = mace_mp(model=\"small\", dispersion=False, default_dtype=\"float64\", device='cpu')\n",
    "calc.models[0] = torch_model\n",
    "atoms.calc = calc\n",
    "print(calc.get_potential_energy(atoms,force_consistent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(torch_model.products[0].symmetric_contractions.contractions[0].correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "model_params = yaml.load(open(model_name+\".yaml\"), Loader=yaml.FullLoader)\n",
    "model_fennol = FENNIX(**model_params,rng_key=jax.random.PRNGKey(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymmetricContraction_0\n",
      "    w1_0e\n",
      "    w2_0e\n",
      "    w3_0e\n",
      "SymmetricContraction_1\n",
      "    w1_0e\n",
      "    w2_0e\n",
      "    w3_0e\n",
      "hidden_linear_readout_last\n",
      "    w[0,0] 128x0e,16x0e\n",
      "linear_contraction_0\n",
      "    w[0,0] 128x0e,128x0e\n",
      "linear_contraction_1\n",
      "    w[0,0] 128x0e,128x0e\n",
      "linear_dn_0\n",
      "    w[0,0] 128x0e,128x0e\n",
      "    w[1,1] 128x1o,128x1o\n",
      "    w[2,2] 128x2e,128x2e\n",
      "    w[3,3] 128x3o,128x3o\n",
      "linear_dn_1\n",
      "    w[0,0] 128x0e,128x0e\n",
      "    w[1,1] 128x1o,128x1o\n",
      "    w[2,2] 128x2e,128x2e\n",
      "    w[3,3] 128x3o,128x3o\n",
      "linear_readout_0\n",
      "    w[0,0] 128x0e,1x0e\n",
      "linear_readout_last\n",
      "    w[0,0] 16x0e,1x0e\n",
      "linear_up_0\n",
      "    w[0,0] 128x0e,128x0e\n",
      "linear_up_1\n",
      "    w[0,0] 128x0e,128x0e\n",
      "radial_network_0\n",
      "    Dense_0\n",
      "    Dense_1\n",
      "    Dense_2\n",
      "    Dense_3\n",
      "radial_network_1\n",
      "    Dense_0\n",
      "    Dense_1\n",
      "    Dense_2\n",
      "    Dense_3\n",
      "skip_tp_0\n",
      "    w[0,0] 128x0e,128x0e\n",
      "skip_tp_1\n",
      "    w[0,0] 128x0e,128x0e\n",
      "species_encoding\n"
     ]
    }
   ],
   "source": [
    "variables = deepcopy(model_fennol.variables)\n",
    "nn_energy_params = variables[\"params\"][\"nn_energy\"]\n",
    "\n",
    "for k in nn_energy_params.keys():\n",
    "    print(k)\n",
    "    if isinstance(nn_energy_params[k],dict):\n",
    "        for kk in nn_energy_params[k].keys():\n",
    "            print(\"   \",kk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_torch_to_jax(linear):\n",
    "    return {\n",
    "        f\"w[{ins.i_in},{ins.i_out}] {linear.irreps_in[ins.i_in]},{linear.irreps_out[ins.i_out]}\": jnp.asarray(\n",
    "            w.data\n",
    "        )\n",
    "        for i, ins, w in linear.weight_views(yield_instruction=True)\n",
    "    }\n",
    "\n",
    "def skip_tp_torch_to_jax(tp):\n",
    "    return {\n",
    "        f\"w[{ins.i_in1},{ins.i_out}] {tp.irreps_in1[ins.i_in1]},{tp.irreps_out[ins.i_out]}\": jnp.moveaxis(\n",
    "            jnp.asarray(w.data), 1, 0\n",
    "        )/num_species**0.5\n",
    "        for i, ins, w in tp.weight_views(yield_instruction=True)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09600671 -0.04891878 -0.12642812 -0.00084473 -0.02929069 -0.00255149\n",
      "  0.09475341  0.1371028  -0.03754604 -0.02659503]\n",
      "[-0.9057255  -0.46149882 -1.1927205  -0.00796918 -0.27632779 -0.02407071\n",
      "  0.89390186  1.29342525 -0.35420867 -0.25089705]\n",
      "[Instruction(i_in=0, i_out=0, path_shape=(89, 128), path_weight=0.105999788000636)]\n"
     ]
    }
   ],
   "source": [
    "## get initial species encodings\n",
    "species_encodings = np.zeros(nn_energy_params[\"species_encoding\"].shape,dtype=np.float64)\n",
    "species_encodings_torch = torch_model.node_embedding.linear.weight.detach().numpy().reshape(num_species,-1)\n",
    "assert species_encodings.shape[1] == species_encodings_torch.shape[1]\n",
    "factor = num_species**-0.5\n",
    "for i,z in enumerate(species_order):\n",
    "    species_encodings[z,:] = species_encodings_torch[i,:]*factor\n",
    "    # print(f\"{z}: {species_encodings[z]}\")\n",
    "            \n",
    "nn_energy_params[\"species_encoding\"] = jnp.array(species_encodings,dtype=jnp.float64)\n",
    "print(species_encodings[1,:10])\n",
    "print(species_encodings_torch[0,:10])\n",
    "print(torch_model.node_embedding.linear.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "89\n",
      "[ 0.00000000e+00 -1.34765940e-01 -4.89536002e-02 -1.27964837e-01\n",
      " -1.74070414e-01 -2.83886143e-01 -3.08899133e-01 -2.70478705e-01\n",
      " -2.67704064e-01 -1.79942722e-01  5.11468097e-14 -1.01404660e-01\n",
      " -1.03414343e-01 -1.78119602e-01 -2.82778434e-01 -2.55896403e-01\n",
      " -1.71716001e-01 -1.03327676e-01 -2.30032546e-03 -9.61966981e-02\n",
      " -1.98095792e-01 -2.89797631e-01 -3.77356485e-01 -3.18438310e-01\n",
      " -3.39308360e-01 -3.05201342e-01 -2.59045481e-01 -2.04967133e-01\n",
      " -1.90094970e-01 -1.19511468e-01 -4.74125486e-02 -1.29617881e-01\n",
      " -1.73032698e-01 -1.46134081e-01 -1.42816358e-01 -9.25529485e-02\n",
      "  2.48680741e-01 -9.42067374e-02 -1.81468346e-01 -3.72998966e-01\n",
      " -4.35363990e-01 -4.46096217e-01 -3.23088239e-01 -3.22914437e-01\n",
      " -2.85943987e-01 -2.51733646e-01 -1.79741637e-01 -7.58296426e-02\n",
      " -2.35037477e-02 -1.02484460e-01 -1.40331120e-01 -1.31822331e-01\n",
      " -1.05852917e-01 -6.01071435e-02  3.61860430e-01 -1.01622332e-01\n",
      " -1.83414267e-01 -3.28306864e-01 -3.21027057e-01 -2.94691568e-01\n",
      " -3.03236739e-01 -2.78990552e-01 -3.00229465e-01 -4.99521216e-01\n",
      " -6.80506441e-01 -2.81036643e-01 -2.98514050e-01 -2.79581030e-01\n",
      " -2.51743551e-01 -2.87632615e-01 -1.31738478e-01 -2.73981126e-01\n",
      " -4.70254749e-01 -5.18464119e-01 -3.43786860e-01 -4.18484290e-01\n",
      " -3.53598655e-01 -2.69166496e-01 -1.94944002e-01 -8.74667898e-02\n",
      "  9.16856059e-03 -8.54046505e-02 -1.37076529e-01 -1.26373289e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.86057344e-01 -4.05147524e-01 -4.50752988e-01\n",
      " -5.09185684e-01 -5.48781014e-01 -5.61633506e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# species energy shift\n",
    "\n",
    "print(len(model_fennol.variables[\"params\"][\"energy_shift\"][\"constant\"]))\n",
    "print(len(torch_model.atomic_energies_fn.atomic_energies))\n",
    "energy_shift = np.zeros(len(model_fennol.variables[\"params\"][\"energy_shift\"][\"constant\"]),dtype=np.float64)\n",
    "for i,z in enumerate(species_order):\n",
    "    # print(f\"{z}: {model_fennol.variables['params']['energy_shift']['constant'][z]} {torch_model.atomic_energies_fn.atomic_energies[i].data/au.EV}\")\n",
    "    energy_shift[z] = torch_model.atomic_energies_fn.atomic_energies[i].data/au.EV\n",
    "variables[\"params\"][\"energy_shift\"][\"constant\"] = jnp.array(energy_shift,dtype=jnp.float64)\n",
    "print(variables[\"params\"][\"energy_shift\"][\"constant\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip_tp_0\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "(89, 128, 128)\n",
      "(120, 128, 128)\n",
      "skip_tp_1\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "(89, 128, 128)\n",
      "(120, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# skip tp 0\n",
    "for layer in range(2):\n",
    "    print(f'skip_tp_{layer}')\n",
    "    skip_params_torch = skip_tp_torch_to_jax(\n",
    "        torch_model.interactions[layer].skip_tp\n",
    "    )\n",
    "    print(skip_params_torch.keys())\n",
    "    print(nn_energy_params[f\"skip_tp_{layer}\"].keys())\n",
    "    for k in nn_energy_params[f'skip_tp_{layer}'].keys():\n",
    "        print(skip_params_torch[k].shape)\n",
    "        print(nn_energy_params[f'skip_tp_{layer}'][k].shape)\n",
    "        skip_params = np.zeros_like(nn_energy_params[f'skip_tp_{layer}'][k])\n",
    "        assert skip_params.shape[1:] == nn_energy_params[f'skip_tp_{layer}'][k].shape[1:]\n",
    "        for i,z in enumerate(species_order):\n",
    "            skip_params[z,:,:] = np.array(skip_params_torch[k][i,:,:])\n",
    "\n",
    "        nn_energy_params[f'skip_tp_{layer}'][k] = jnp.array(skip_params,dtype=jnp.float64)\n",
    "\n",
    "# # skip tp 1\n",
    "# print()\n",
    "# print(\"skip tp 1\")\n",
    "# skip_params_torch = skip_tp_torch_to_jax(\n",
    "#         torch_model.interactions[1].skip_tp\n",
    "#     )\n",
    "# print(skip_params_torch.keys())\n",
    "# print(nn_energy_params[\"skip_tp_1\"].keys())\n",
    "\n",
    "# print(skip_params_torch['w[0,0] 128x0e,128x0e'].shape)\n",
    "# print(nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'].shape)\n",
    "# skip_params = np.zeros_like(nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'])\n",
    "# assert skip_params.shape[1:] == nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'].shape[1:]\n",
    "# for i,z in enumerate(species_order):\n",
    "#     skip_params[z,:,:] = np.array(skip_params_torch['w[0,0] 128x0e,128x0e'][i,:,:])\n",
    "\n",
    "# nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'] = jnp.array(skip_params,dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "linear_up_0\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "w[0,0] 128x0e,128x0e\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "\n",
      "linear_up_1\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "w[0,0] 128x0e,128x0e\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "\n",
      "linear_dn_0\n",
      "dict_keys(['w[0,0] 128x0e,128x0e', 'w[1,1] 128x1o,128x1o', 'w[2,2] 128x2e,128x2e', 'w[3,3] 128x3o,128x3o'])\n",
      "dict_keys(['w[0,0] 128x0e,128x0e', 'w[1,1] 128x1o,128x1o', 'w[2,2] 128x2e,128x2e', 'w[3,3] 128x3o,128x3o'])\n",
      "w[0,0] 128x0e,128x0e\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "w[1,1] 128x1o,128x1o\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "w[2,2] 128x2e,128x2e\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "w[3,3] 128x3o,128x3o\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "\n",
      "linear_dn_1\n",
      "dict_keys(['w[0,0] 128x0e,128x0e', 'w[1,1] 128x1o,128x1o', 'w[2,2] 128x2e,128x2e', 'w[3,3] 128x3o,128x3o'])\n",
      "dict_keys(['w[0,0] 128x0e,128x0e', 'w[1,1] 128x1o,128x1o', 'w[2,2] 128x2e,128x2e', 'w[3,3] 128x3o,128x3o'])\n",
      "w[0,0] 128x0e,128x0e\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "w[1,1] 128x1o,128x1o\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "w[2,2] 128x2e,128x2e\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "w[3,3] 128x3o,128x3o\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "\n",
      "linear_contraction_0\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "w[0,0] 128x0e,128x0e\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "\n",
      "linear_contraction_1\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "dict_keys(['w[0,0] 128x0e,128x0e'])\n",
      "w[0,0] 128x0e,128x0e\n",
      "(128, 128)\n",
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "# linear up\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print(f\"linear_up_{i}\")\n",
    "    up_params_torch = linear_torch_to_jax(torch_model.interactions[i].linear_up)\n",
    "    print(up_params_torch.keys())\n",
    "    print(nn_energy_params[f\"linear_up_{i}\"].keys())\n",
    "    for k in up_params_torch.keys():\n",
    "        print(k)\n",
    "        print(up_params_torch[k].shape)\n",
    "        print(nn_energy_params[f\"linear_up_{i}\"][k].shape)\n",
    "        assert up_params_torch[k].shape == nn_energy_params[f\"linear_up_{i}\"][k].shape\n",
    "        nn_energy_params[f\"linear_up_{i}\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "# linear down\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print(f\"linear_dn_{i}\")\n",
    "    up_params_torch = linear_torch_to_jax(torch_model.interactions[i].linear)\n",
    "    print(up_params_torch.keys())\n",
    "    print(nn_energy_params[f\"linear_dn_{i}\"].keys())\n",
    "    for k in up_params_torch.keys():\n",
    "        print(k)\n",
    "        print(up_params_torch[k].shape)\n",
    "        print(nn_energy_params[f\"linear_dn_{i}\"][k].shape)\n",
    "        assert up_params_torch[k].shape == nn_energy_params[f\"linear_dn_{i}\"][k].shape\n",
    "        nn_energy_params[f\"linear_dn_{i}\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "# contraction linear\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print(f\"linear_contraction_{i}\")\n",
    "    up_params_torch = linear_torch_to_jax(torch_model.products[i].linear)\n",
    "    print(up_params_torch.keys())\n",
    "    print(nn_energy_params[f\"linear_contraction_{i}\"].keys())\n",
    "    for k in up_params_torch.keys():\n",
    "        print(k)\n",
    "        print(up_params_torch[k].shape)\n",
    "        try:\n",
    "            print(nn_energy_params[f\"linear_contraction_{i}\"][k].shape)\n",
    "            assert up_params_torch[k].shape == nn_energy_params[f\"linear_contraction_{i}\"][k].shape\n",
    "            nn_energy_params[f\"linear_contraction_{i}\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "        except KeyError:\n",
    "            print(f\"key {k} not found in nn_energy_params[f'linear_contraction_{i}']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_readout_0\n",
      "dict_keys(['w[0,0] 128x0e,1x0e'])\n",
      "dict_keys(['w[0,0] 128x0e,1x0e'])\n",
      "w[0,0] 128x0e,1x0e\n",
      "(128, 1)\n",
      "(128, 1)\n",
      "\n",
      "hidden_linear_readout_last\n",
      "dict_keys(['w[0,0] 128x0e,16x0e'])\n",
      "dict_keys(['w[0,0] 128x0e,16x0e'])\n",
      "w[0,0] 128x0e,16x0e\n",
      "(128, 16)\n",
      "(128, 16)\n",
      "silu_norm 1.6765620560798837 1.6765647703084872\n",
      "\n",
      "linear_readout_last\n",
      "dict_keys(['w[0,0] 16x0e,1x0e'])\n",
      "dict_keys(['w[0,0] 16x0e,1x0e'])\n",
      "w[0,0] 16x0e,1x0e\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "1.6791767923989418\n"
     ]
    }
   ],
   "source": [
    "# readout block\n",
    "print(f\"linear_readout_0\")\n",
    "up_params_torch = linear_torch_to_jax(torch_model.readouts[0].linear)\n",
    "print(up_params_torch.keys())\n",
    "print(nn_energy_params[f\"linear_readout_0\"].keys())\n",
    "for k in up_params_torch.keys():\n",
    "    print(k)\n",
    "    print(up_params_torch[k].shape)\n",
    "    print(nn_energy_params[f\"linear_readout_0\"][k].shape)\n",
    "    assert up_params_torch[k].shape == nn_energy_params[f\"linear_readout_0\"][k].shape\n",
    "    nn_energy_params[f\"linear_readout_0\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "print()\n",
    "print(f\"hidden_linear_readout_last\")\n",
    "up_params_torch = linear_torch_to_jax(torch_model.readouts[1].linear_1)\n",
    "print(up_params_torch.keys())\n",
    "print(nn_energy_params[f\"hidden_linear_readout_last\"].keys())\n",
    "for k in up_params_torch.keys():\n",
    "    print(k)\n",
    "    print(up_params_torch[k].shape)\n",
    "    print(nn_energy_params[f\"hidden_linear_readout_last\"][k].shape)\n",
    "    assert up_params_torch[k].shape == nn_energy_params[f\"hidden_linear_readout_last\"][k].shape\n",
    "    nn_energy_params[f\"hidden_linear_readout_last\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "\n",
    "f,silu_norm = normalize_activation(jax.nn.silu,return_scale=True)\n",
    "print(\"silu_norm\",silu_norm,1.6765647703084872)\n",
    "cst = torch_model.readouts[1].non_linearity.acts[0].cst / silu_norm\n",
    "\n",
    "print()\n",
    "print(f\"linear_readout_last\")\n",
    "up_params_torch = linear_torch_to_jax(torch_model.readouts[1].linear_2)\n",
    "print(up_params_torch.keys())\n",
    "print(nn_energy_params[f\"linear_readout_last\"].keys())\n",
    "for k in up_params_torch.keys():\n",
    "    print(k)\n",
    "    print(up_params_torch[k].shape)\n",
    "    print(nn_energy_params[f\"linear_readout_last\"][k].shape)\n",
    "    assert up_params_torch[k].shape == nn_energy_params[f\"linear_readout_last\"][k].shape\n",
    "    nn_energy_params[f\"linear_readout_last\"][k] = cst*jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "print(torch_model.readouts[1].non_linearity.acts[0].cst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radial net 0\n",
      "dict_keys(['Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])\n",
      "(10, 64) (10, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 512) (64, 512)\n",
      "\n",
      "radial net 1\n",
      "dict_keys(['Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])\n",
      "(10, 64) (10, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 512) (64, 512)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# radial net\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"radial net\",i)\n",
    "    print(nn_energy_params[f\"radial_network_{i}\"].keys())\n",
    "    w  = torch_model.interactions[i].conv_tp_weights.layer0.weight.detach().numpy()\n",
    "    w = w/w.shape[0]**0.5\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_0\"][\"kernel\"].shape)\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_0\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_0\"][\"kernel\"] = jnp.array(w,dtype=jnp.float64)\n",
    "\n",
    "    cst = torch_model.interactions[i].conv_tp_weights.layer0.act.cst/ silu_norm\n",
    "    print(\"cst\",cst)\n",
    "\n",
    "    w  = cst*torch_model.interactions[i].conv_tp_weights.layer1.weight.detach().numpy()\n",
    "    w = w/w.shape[0]**0.5\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_1\"][\"kernel\"].shape)\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_1\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_1\"][\"kernel\"] =  jnp.array(w,dtype=jnp.float64)\n",
    "\n",
    "    cst = torch_model.interactions[i].conv_tp_weights.layer0.act.cst/ silu_norm\n",
    "    print(\"cst\",cst)\n",
    "\n",
    "    w  = cst*torch_model.interactions[i].conv_tp_weights.layer2.weight.detach().numpy()\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_2\"][\"kernel\"].shape)\n",
    "    w = w/w.shape[0]**0.5\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_2\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_2\"][\"kernel\"] =  jnp.array(w,dtype=jnp.float64)\n",
    "\n",
    "    cst = torch_model.interactions[i].conv_tp_weights.layer0.act.cst/ silu_norm\n",
    "    print(\"cst\",cst)\n",
    "\n",
    "    w  = cst*torch_model.interactions[i].conv_tp_weights.layer3.weight.detach().numpy()\n",
    "    w = w/w.shape[0]**0.5\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_3\"][\"kernel\"].shape)\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_3\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_3\"][\"kernel\"] =  jnp.array(w,dtype=jnp.float64)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mace.tools.cg import U_matrix_real\n",
    "from e3nn_jax import reduced_tensor_product_basis\n",
    "from e3nn import o3\n",
    "import e3nn_jax\n",
    "\n",
    "def get_reordering(order,ir_out):\n",
    "    # torch \n",
    "    irreps_in = o3.Irreps(\"1x0e + 1x1o + 1x2e + 1x3o\")\n",
    "    irrep_out = o3.Irreps(\"1x\"+ir_out)\n",
    "    U_matrix = U_matrix_real(irreps_in=irreps_in, irreps_out=irrep_out, correlation=order)[-1].numpy()\n",
    "    U_matrix_abs = np.abs(U_matrix)\n",
    "\n",
    "    # jax\n",
    "    irreps_in = e3nn_jax.Irreps(\"1x0e + 1x1o + 1x2e + 1x3o\")\n",
    "    irrep_out = e3nn_jax.Irrep(ir_out)\n",
    "    U = reduced_tensor_product_basis([irreps_in] * order, keep_ir=[irrep_out]).list[-1]\n",
    "    U = np.moveaxis(U,-1,0)\n",
    "    U_abs = np.abs(U)\n",
    "\n",
    "    index = []\n",
    "    signs = []\n",
    "    for i in range(U.shape[-1]):\n",
    "        diffs = []\n",
    "        for k in range(U_matrix.shape[-1]):\n",
    "            diff = np.max(np.abs(U_abs[...,i]-U_matrix_abs[...,k]))\n",
    "            diffs.append(diff)\n",
    "        k = np.argmin(diffs)\n",
    "        diff = diffs[k]\n",
    "        if diff>1.e-6:\n",
    "            print(i,k,diff)\n",
    "            raise ValueError(\"diff too large\")\n",
    "        \n",
    "        index.append(k)\n",
    "        diff = np.max(np.abs(U[...,i]-U_matrix[...,k]))\n",
    "        if diff < 1.e-6:\n",
    "            signs.append(1.)\n",
    "        else:\n",
    "            diff = np.max(np.abs(U[...,i]+U_matrix[...,k]))\n",
    "            if diff < 1.e-6:\n",
    "                print(i,k,\"global sign differ\")\n",
    "                signs.append(-1.)\n",
    "            else:\n",
    "                raise ValueError(\"local sign differ\")\n",
    "    print(order,ir_out,len(index),U.shape[-1],len(signs))\n",
    "    return np.array(index),np.array(signs)\n",
    "    return np.arange(U.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layer 0 -> ['w1_0e', 'w2_0e', 'w3_0e']\n",
      "   ir 0e l 0 corr 3\n",
      "     w3_0e (89, 23, 128) (120, 23, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0e 23 23 23\n",
      "0.12137811837893642\n",
      "     w2_0e (89, 4, 128) (120, 4, 128)\n",
      "2 0e 4 4 4\n",
      "0.27486145801722217\n",
      "     w1_0e (89, 1, 128) (120, 1, 128)\n",
      "1 0e 1 1 1\n",
      "0.468239133582122\n",
      "\n",
      "layer 1 -> ['w1_0e', 'w2_0e', 'w3_0e']\n",
      "   ir 0e l 0 corr 3\n",
      "     w3_0e (89, 23, 128) (120, 23, 128)\n",
      "3 0e 23 23 23\n",
      "0.0098936799287686\n",
      "     w2_0e (89, 4, 128) (120, 4, 128)\n",
      "2 0e 4 4 4\n",
      "0.08516793992552044\n",
      "     w1_0e (89, 1, 128) (120, 1, 128)\n",
      "1 0e 1 1 1\n",
      "0.32557432233660294\n"
     ]
    }
   ],
   "source": [
    "ir_layers = [[\"0e\"],[\"0e\"]]\n",
    "for layer in [0,1]:\n",
    "    print()\n",
    "    print(\"layer\",layer,\"->\",list(nn_energy_params[f\"SymmetricContraction_{layer}\"].keys()))\n",
    "\n",
    "    for ir in ir_layers[layer]:\n",
    "        l=int(ir[0])\n",
    "        corr = len(torch_model.products[layer].symmetric_contractions.contractions[l].weights)+1\n",
    "        print(\"   ir\",ir,\"l\",l,\"corr\",corr)\n",
    "\n",
    "        w=torch_model.products[layer].symmetric_contractions.contractions[l].weights_max.detach().cpu().numpy()\n",
    "\n",
    "        print(f\"     w{corr}_{ir}\",w.shape,nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"].shape) \n",
    "        assert w.shape[1:] == nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"].shape[1:]\n",
    "\n",
    "        # reorder weights\n",
    "        index,signs = get_reordering(corr,ir)\n",
    "        w = w[:,index,:]*signs[None,:,None]\n",
    "\n",
    "\n",
    "        # last order\n",
    "        skip_params = np.zeros_like(nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"])\n",
    "        for i,z in enumerate(species_order):\n",
    "            skip_params[z,:,:] = w[i,:,:]\n",
    "        nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"] = jnp.asarray(skip_params,dtype=jnp.float64)\n",
    "\n",
    "        print(np.abs(w).mean())\n",
    "\n",
    "        for c in range(corr-1):\n",
    "            order = corr-c-1\n",
    "            w=torch_model.products[layer].symmetric_contractions.contractions[l].weights[c].detach().cpu().numpy()\n",
    "            print(f\"     w{order}_{ir}\",w.shape,nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"].shape)\n",
    "            assert w.shape[1:] == nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"].shape[1:]\n",
    "\n",
    "            index,signs = get_reordering(order,ir)\n",
    "            w = w[:,index,:]*signs[None,:,None]\n",
    "\n",
    "            skip_params = np.zeros_like(nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"])\n",
    "            for i,z in enumerate(species_order):\n",
    "                skip_params[z,:,:] = w[i,:,:]\n",
    "            nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"] = jnp.asarray(skip_params,dtype=jnp.float64)\n",
    "            print(np.abs(w).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACE fennol -13.883318033770568\n",
      "MACE torch -13.883318032136243\n"
     ]
    }
   ],
   "source": [
    "variables[\"params\"][\"nn_energy\"] = nn_energy_params  \n",
    "model_fennol.variables = variables \n",
    "\n",
    "model_fennol.save(model_name+\".fnx\")\n",
    "model_fennol_load = FENNIX.load(model_name+\".fnx\")\n",
    "# print(\"##############################\")\n",
    "# print(\"Restored model\")\n",
    "# for k in model_fennol_load.variables[\"params\"][\"nn_energy\"].keys():\n",
    "#     print(k)\n",
    "#     if isinstance(nn_energy_params[k],dict):\n",
    "#         for kk in nn_energy_params[k].keys():\n",
    "#             print(\"   \",kk)\n",
    "calc_fennol = FENNIXCalculator(model=model_fennol_load,atoms=atoms,use_float64=True)\n",
    "atoms.calc = calc_fennol\n",
    "print(\"MACE fennol\",atoms.get_potential_energy())\n",
    "atoms.calc = calc  \n",
    "print(\"MACE torch\",atoms.get_potential_energy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['batch_index', 'coordinates', 'energy_shift', 'graph', 'natoms', 'nn_energy', 'nn_energy_node_feats', 'species'])\n",
      "[0.13850512 0.29855576 0.29855576]\n"
     ]
    }
   ],
   "source": [
    "species = atoms.get_atomic_numbers()\n",
    "coordinates = atoms.get_positions()\n",
    "output = model_fennol_load(coordinates=coordinates,species=species)\n",
    "print(output.keys())\n",
    "print(output[\"nn_energy\"]*au.EV)\n",
    "# print(output[\"nn_energy_node_feats\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13850512 0.29855576 0.29855576]\n"
     ]
    }
   ],
   "source": [
    "node_feats_torch = calc.get_descriptors(atoms)\n",
    "# print(node_feats_torch[0])\n",
    "print(calc.get_property(\"node_energy\",atoms=atoms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
