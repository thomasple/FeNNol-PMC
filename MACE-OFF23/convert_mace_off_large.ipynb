{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.211386024367243\n",
      "27.211386024367243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '' \n",
    "import jax\n",
    "\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "\n",
    "from mace.calculators import mace_mp,mace_off\n",
    "from ase import build\n",
    "from fennol import FENNIX\n",
    "import yaml\n",
    "import jax.numpy as jnp\n",
    "import numpy as np  \n",
    "from fennol.ase import FENNIXCalculator\n",
    "from fennol.utils import AtomicUnits as au\n",
    "from copy import deepcopy\n",
    "from mace import modules as torch_modules\n",
    "import e3nn as e3nn_torch\n",
    "import torch\n",
    "from fennol.utils.activations import normalize_activation\n",
    "\n",
    "from ase.units import Hartree\n",
    "print(au.EV)\n",
    "print(Hartree)\n",
    "\n",
    "e3nn_torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MACE-OFF23 MODEL for MACECalculator with /home/pthomas/.cache/mace/MACE-OFF23_large.model\n",
      "Using float64 for MACECalculator, which is slower but more accurate. Recommended for geometry optimization.\n",
      "-2080.921473483096\n",
      "species_order: [1, 6, 7, 8, 9, 15, 16, 17, 35, 53]\n",
      "53\n",
      "10\n",
      "scale 0.040001721591467386\n",
      "shift 0.0\n",
      "avg_num_neighbors 18.41771125793457\n",
      "ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(10x0e -> 224x0e | 2240 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=5.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=5.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-13.5720, -1030.5672, -1486.3750, -2043.9337, -2715.3185, -9287.4071, -10834.4845, -12522.6493, -70045.2839, -8102.5246])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(224x0e -> 224x0e | 50176 weights)\n",
      "      (conv_tp): TensorProduct(224x0e x 1x0e+1x1o+1x2e+1x3o -> 224x0e+224x1o+224x2e+224x3o | 896 paths | 896 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 896]\n",
      "      (linear): Linear(224x0e+224x1o+224x2e+224x3o -> 224x0e+224x1o+224x2e+224x3o | 200704 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(224x0e+224x1o+224x2e+224x3o x 10x0e -> 224x0e+224x1o+224x2e+224x3o | 2007040 paths | 2007040 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(224x0e+224x1o+224x2e -> 224x0e+224x1o+224x2e | 150528 weights)\n",
      "      (conv_tp): TensorProduct(224x0e+224x1o+224x2e x 1x0e+1x1o+1x2e+1x3o -> 672x0e+1120x1o+1120x2e+896x3o | 3808 paths | 3808 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 3808]\n",
      "      (linear): Linear(672x0e+1120x1o+1120x2e+896x3o -> 224x0e+224x1o+224x2e+224x3o | 852992 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(224x0e+224x1o+224x2e x 10x0e -> 224x0e | 501760 paths | 501760 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 10x4x224]\n",
      "                (1): Parameter containing: [torch.float64 of size 10x1x224]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 10x6x224]\n",
      "                (1): Parameter containing: [torch.float64 of size 10x1x224]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (2): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 10x7x224]\n",
      "                (1): Parameter containing: [torch.float64 of size 10x1x224]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(224x0e+224x1o+224x2e -> 224x0e+224x1o+224x2e | 150528 weights)\n",
      "    )\n",
      "    (1): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 10x4x224]\n",
      "                (1): Parameter containing: [torch.float64 of size 10x1x224]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(224x0e -> 224x0e | 50176 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(224x0e+224x1o+224x2e -> 1x0e | 224 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(224x0e -> 16x0e | 3584 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=1.088502, shift=0.000000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "atoms = build.molecule('H2O')\n",
    "model_name = \"mace_off_large\"\n",
    "atoms.set_positions([[0.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0]])\n",
    "calc = mace_off(model=\"large\", default_dtype=\"float64\", device='cpu')\n",
    "atoms.calc = calc\n",
    "print(atoms.get_potential_energy())\n",
    "\n",
    "torch_model_mp = calc.models[0].cpu()\n",
    "species_order = torch_model_mp.atomic_numbers.tolist()\n",
    "print(\"species_order:\",species_order)\n",
    "print(max(species_order))\n",
    "num_species = len(species_order)\n",
    "print(num_species)\n",
    "\n",
    "scale = torch_model_mp.scale_shift.scale.item()\n",
    "shift = torch_model_mp.scale_shift.shift.item()\n",
    "print(\"scale\",scale/au.EV)\n",
    "print(\"shift\",shift/au.EV)\n",
    "\n",
    "avg_num_neigh = torch_model_mp.interactions[0].avg_num_neighbors\n",
    "print(\"avg_num_neighbors\",avg_num_neigh)\n",
    "\n",
    "print(torch_model_mp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6791767923989418\n",
      "r_max=5.0,num_basis=8,p=5.0,max_ell=3,num_inter=2,num_species=10,avg_num_neigh=18.41771125793457,corr=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/mace/modules/models.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"atomic_numbers\", torch.tensor(atomic_numbers, dtype=torch.int64)\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/mace/modules/blocks.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(atomic_energies, dtype=torch.get_default_dtype()),\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(10x0e -> 224x0e | 2240 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=5.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=5.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-13.5720, -1030.5672, -1486.3750, -2043.9337, -2715.3185, -9287.4071, -10834.4845, -12522.6493, -70045.2839, -8102.5246])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(224x0e -> 224x0e | 50176 weights)\n",
      "      (conv_tp): TensorProduct(224x0e x 1x0e+1x1o+1x2e+1x3o -> 224x0e+224x1o+224x2e+224x3o | 896 paths | 896 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 896]\n",
      "      (linear): Linear(224x0e+224x1o+224x2e+224x3o -> 224x0e+224x1o+224x2e+224x3o | 200704 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(224x0e+224x1o+224x2e+224x3o x 10x0e -> 224x0e+224x1o+224x2e+224x3o | 2007040 paths | 2007040 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(224x0e+224x1o+224x2e -> 224x0e+224x1o+224x2e | 150528 weights)\n",
      "      (conv_tp): TensorProduct(224x0e+224x1o+224x2e x 1x0e+1x1o+1x2e+1x3o -> 672x0e+1120x1o+1120x2e+896x3o | 3808 paths | 3808 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 3808]\n",
      "      (linear): Linear(672x0e+1120x1o+1120x2e+896x3o -> 224x0e+224x1o+224x2e+224x3o | 852992 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(224x0e+224x1o+224x2e x 10x0e -> 224x0e | 501760 paths | 501760 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 10x4x224]\n",
      "                (1): Parameter containing: [torch.float64 of size 10x1x224]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 10x6x224]\n",
      "                (1): Parameter containing: [torch.float64 of size 10x1x224]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (2): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 10x7x224]\n",
      "                (1): Parameter containing: [torch.float64 of size 10x1x224]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(224x0e+224x1o+224x2e -> 224x0e+224x1o+224x2e | 150528 weights)\n",
      "    )\n",
      "    (1): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 10x4x224]\n",
      "                (1): Parameter containing: [torch.float64 of size 10x1x224]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(224x0e -> 224x0e | 50176 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(224x0e+224x1o+224x2e -> 1x0e | 224 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(224x0e -> 16x0e | 3584 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=1.088502, shift=0.000000)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/pthomas/miniconda3/envs/mace/lib/python3.10/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "r_max = torch_model_mp.radial_embedding.bessel_fn.r_max.item()\n",
    "num_basis = torch_model_mp.radial_embedding.bessel_fn.bessel_weights.numel()\n",
    "p = torch_model_mp.radial_embedding.cutoff_fn.p.item()\n",
    "max_ell = torch_model_mp.spherical_harmonics._lmax\n",
    "num_inter = len(torch_model_mp.products)\n",
    "corr = len(torch_model_mp.products[0].symmetric_contractions.contractions[0].weights)+1\n",
    "print(torch_model_mp.readouts[1].non_linearity.acts[0].cst)\n",
    "print(f\"{r_max=},{num_basis=},{p=},{max_ell=},{num_inter=},{num_species=},{avg_num_neigh=},{corr=}\")\n",
    "torch_model = torch_modules.ScaleShiftMACE(\n",
    "    atomic_inter_scale=scale,\n",
    "    atomic_inter_shift=shift,\n",
    "    r_max=r_max,\n",
    "    num_bessel=num_basis,\n",
    "    num_polynomial_cutoff=p,\n",
    "    max_ell=max_ell,\n",
    "    interaction_cls_first=torch_modules.RealAgnosticInteractionBlock,\n",
    "    interaction_cls=torch_modules.RealAgnosticResidualInteractionBlock,\n",
    "    num_interactions=num_inter,\n",
    "    num_elements=num_species,\n",
    "    hidden_irreps=e3nn_torch.o3.Irreps(\"224x0e+224x1o+224x2e\"),\n",
    "    MLP_irreps=\"16x0e\",\n",
    "    avg_num_neighbors=avg_num_neigh,\n",
    "    correlation=corr,\n",
    "    atomic_energies=torch_model_mp.atomic_energies_fn.atomic_energies.clone().detach(),\n",
    "    atomic_numbers=torch_model_mp.atomic_numbers.clone().detach(),\n",
    "    gate=torch.nn.SiLU(),\n",
    ").double()\n",
    "print(torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MACE-OFF23 MODEL for MACECalculator with /home/pthomas/.cache/mace/MACE-OFF23_large.model\n",
      "Using float64 for MACECalculator, which is slower but more accurate. Recommended for geometry optimization.\n",
      "-2080.921473483096\n"
     ]
    }
   ],
   "source": [
    "torch_model.load_state_dict(torch_model_mp.state_dict())\n",
    "calc = mace_off(model=\"large\", default_dtype=\"float64\", device='cpu')\n",
    "calc.models[0] = torch_model\n",
    "atoms.calc = calc\n",
    "print(calc.get_potential_energy(atoms,force_consistent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "model_params = yaml.load(open(model_name+\".yaml\"), Loader=yaml.FullLoader)\n",
    "model_fennol = FENNIX(**model_params,rng_key=jax.random.PRNGKey(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymmetricContraction_0\n",
      "    w1_0e\n",
      "    w1_1o\n",
      "    w1_2e\n",
      "    w2_0e\n",
      "    w2_1o\n",
      "    w2_2e\n",
      "    w3_0e\n",
      "    w3_1o\n",
      "    w3_2e\n",
      "SymmetricContraction_1\n",
      "    w1_0e\n",
      "    w2_0e\n",
      "    w3_0e\n",
      "hidden_linear_readout_last\n",
      "    w[0,0] 224x0e,16x0e\n",
      "linear_contraction_0\n",
      "    w[0,0] 224x0e,224x0e\n",
      "    w[1,1] 224x1o,224x1o\n",
      "    w[2,2] 224x2e,224x2e\n",
      "linear_contraction_1\n",
      "    w[0,0] 224x0e,224x0e\n",
      "linear_dn_0\n",
      "    w[0,0] 224x0e,224x0e\n",
      "    w[1,1] 224x1o,224x1o\n",
      "    w[2,2] 224x2e,224x2e\n",
      "    w[3,3] 224x3o,224x3o\n",
      "linear_dn_1\n",
      "    w[0,0] 672x0e,224x0e\n",
      "    w[1,1] 1120x1o,224x1o\n",
      "    w[2,2] 1120x2e,224x2e\n",
      "    w[3,3] 896x3o,224x3o\n",
      "linear_readout_0\n",
      "    w[0,0] 224x0e,1x0e\n",
      "linear_readout_last\n",
      "    w[0,0] 16x0e,1x0e\n",
      "linear_up_0\n",
      "    w[0,0] 224x0e,224x0e\n",
      "linear_up_1\n",
      "    w[0,0] 224x0e,224x0e\n",
      "    w[1,1] 224x1o,224x1o\n",
      "    w[2,2] 224x2e,224x2e\n",
      "radial_network_0\n",
      "    Dense_0\n",
      "    Dense_1\n",
      "    Dense_2\n",
      "    Dense_3\n",
      "radial_network_1\n",
      "    Dense_0\n",
      "    Dense_1\n",
      "    Dense_2\n",
      "    Dense_3\n",
      "skip_tp_0\n",
      "    w[0,0] 224x0e,224x0e\n",
      "    w[1,1] 224x1o,224x1o\n",
      "    w[2,2] 224x2e,224x2e\n",
      "    w[3,3] 224x3o,224x3o\n",
      "skip_tp_1\n",
      "    w[0,0] 224x0e,224x0e\n",
      "species_encoding\n"
     ]
    }
   ],
   "source": [
    "variables = deepcopy(model_fennol.variables)\n",
    "nn_energy_params = variables[\"params\"][\"nn_energy\"]\n",
    "\n",
    "for k in nn_energy_params.keys():\n",
    "    print(k)\n",
    "    if isinstance(nn_energy_params[k],dict):\n",
    "        for kk in nn_energy_params[k].keys():\n",
    "            print(\"   \",kk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_torch_to_jax(linear):\n",
    "    return {\n",
    "        f\"w[{ins.i_in},{ins.i_out}] {linear.irreps_in[ins.i_in]},{linear.irreps_out[ins.i_out]}\": jnp.asarray(\n",
    "            w.data\n",
    "        )\n",
    "        for i, ins, w in linear.weight_views(yield_instruction=True)\n",
    "    }\n",
    "\n",
    "def skip_tp_torch_to_jax(tp):\n",
    "    return {\n",
    "        f\"w[{ins.i_in1},{ins.i_out}] {tp.irreps_in1[ins.i_in1]},{tp.irreps_out[ins.i_out]}\": jnp.moveaxis(\n",
    "            jnp.asarray(w.data), 1, 0\n",
    "        )/num_species**0.5\n",
    "        for i, ins, w in tp.weight_views(yield_instruction=True)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 224) (10, 224)\n",
      "[ 0.00068487 -0.00089906 -0.00012659 -0.00030575  0.00172688 -0.00110494\n",
      "  0.00195346 -0.00072814 -0.00045683  0.01436365]\n",
      "[ 0.00216575 -0.00284308 -0.00040032 -0.00096685  0.00546088 -0.00349414\n",
      "  0.00617739 -0.00230257 -0.00144461  0.04542185]\n",
      "[Instruction(i_in=0, i_out=0, path_shape=(10, 224), path_weight=0.31622776601683794)]\n"
     ]
    }
   ],
   "source": [
    "## get initial species encodings\n",
    "species_encodings = np.zeros(nn_energy_params[\"species_encoding\"].shape,dtype=np.float64)\n",
    "species_encodings_torch = torch_model.node_embedding.linear.weight.detach().numpy().reshape(num_species,-1)\n",
    "print(species_encodings.shape,species_encodings_torch.shape)\n",
    "assert species_encodings.shape[1] == species_encodings_torch.shape[1]\n",
    "factor = num_species**-0.5\n",
    "for i,z in enumerate(species_order):\n",
    "    species_encodings[z,:] = species_encodings_torch[i,:]*factor\n",
    "    # print(f\"{z}: {species_encodings[z]}\")\n",
    "            \n",
    "nn_energy_params[\"species_encoding\"] = jnp.array(species_encodings,dtype=jnp.float64)\n",
    "print(species_encodings[1,:10])\n",
    "print(species_encodings_torch[0,:10])\n",
    "print(torch_model.node_embedding.linear.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "10\n",
      "[ 0.00000000e+00 -4.98760510e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -3.78726451e+01 -5.46232751e+01\n",
      " -7.51131784e+01 -9.97861162e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -3.41305920e+02\n",
      " -3.98159964e+02 -4.60198876e+02  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.57411672e+03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -2.97762289e+02  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# species energy shift\n",
    "\n",
    "print(len(model_fennol.variables[\"params\"][\"energy_shift\"][\"constant\"]))\n",
    "print(len(torch_model.atomic_energies_fn.atomic_energies))\n",
    "energy_shift = np.zeros(len(model_fennol.variables[\"params\"][\"energy_shift\"][\"constant\"]),dtype=np.float64)\n",
    "for i,z in enumerate(species_order):\n",
    "    # print(f\"{z}: {model_fennol.variables['params']['energy_shift']['constant'][z]} {torch_model.atomic_energies_fn.atomic_energies[i].data/au.EV}\")\n",
    "    energy_shift[z] = torch_model.atomic_energies_fn.atomic_energies[i].data/au.EV\n",
    "variables[\"params\"][\"energy_shift\"][\"constant\"] = jnp.array(energy_shift,dtype=jnp.float64)\n",
    "print(variables[\"params\"][\"energy_shift\"][\"constant\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip_tp_0\n",
      "dict_keys(['w[0,0] 224x0e,224x0e', 'w[1,1] 224x1o,224x1o', 'w[2,2] 224x2e,224x2e', 'w[3,3] 224x3o,224x3o'])\n",
      "dict_keys(['w[0,0] 224x0e,224x0e', 'w[1,1] 224x1o,224x1o', 'w[2,2] 224x2e,224x2e', 'w[3,3] 224x3o,224x3o'])\n",
      "(10, 224, 224)\n",
      "(120, 224, 224)\n",
      "(10, 224, 224)\n",
      "(120, 224, 224)\n",
      "(10, 224, 224)\n",
      "(120, 224, 224)\n",
      "(10, 224, 224)\n",
      "(120, 224, 224)\n",
      "skip_tp_1\n",
      "dict_keys(['w[0,0] 224x0e,224x0e'])\n",
      "dict_keys(['w[0,0] 224x0e,224x0e'])\n",
      "(10, 224, 224)\n",
      "(120, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# skip tp 0\n",
    "for layer in range(2):\n",
    "    print(f'skip_tp_{layer}')\n",
    "    skip_params_torch = skip_tp_torch_to_jax(\n",
    "        torch_model.interactions[layer].skip_tp\n",
    "    )\n",
    "    print(skip_params_torch.keys())\n",
    "    print(nn_energy_params[f\"skip_tp_{layer}\"].keys())\n",
    "    for k in nn_energy_params[f'skip_tp_{layer}'].keys():\n",
    "        print(skip_params_torch[k].shape)\n",
    "        print(nn_energy_params[f'skip_tp_{layer}'][k].shape)\n",
    "        skip_params = np.zeros_like(nn_energy_params[f'skip_tp_{layer}'][k])\n",
    "        assert skip_params.shape[1:] == nn_energy_params[f'skip_tp_{layer}'][k].shape[1:]\n",
    "        for i,z in enumerate(species_order):\n",
    "            skip_params[z,:,:] = np.array(skip_params_torch[k][i,:,:])\n",
    "\n",
    "        nn_energy_params[f'skip_tp_{layer}'][k] = jnp.array(skip_params,dtype=jnp.float64)\n",
    "\n",
    "# # skip tp 1\n",
    "# print()\n",
    "# print(\"skip tp 1\")\n",
    "# skip_params_torch = skip_tp_torch_to_jax(\n",
    "#         torch_model.interactions[1].skip_tp\n",
    "#     )\n",
    "# print(skip_params_torch.keys())\n",
    "# print(nn_energy_params[\"skip_tp_1\"].keys())\n",
    "\n",
    "# print(skip_params_torch['w[0,0] 128x0e,128x0e'].shape)\n",
    "# print(nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'].shape)\n",
    "# skip_params = np.zeros_like(nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'])\n",
    "# assert skip_params.shape[1:] == nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'].shape[1:]\n",
    "# for i,z in enumerate(species_order):\n",
    "#     skip_params[z,:,:] = np.array(skip_params_torch['w[0,0] 128x0e,128x0e'][i,:,:])\n",
    "\n",
    "# nn_energy_params[\"skip_tp_1\"]['w[0,0] 128x0e,128x0e'] = jnp.array(skip_params,dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "linear_up_0\n",
      "dict_keys(['w[0,0] 224x0e,224x0e'])\n",
      "dict_keys(['w[0,0] 224x0e,224x0e'])\n",
      "w[0,0] 224x0e,224x0e\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "\n",
      "linear_up_1\n",
      "dict_keys(['w[0,0] 224x0e,224x0e', 'w[1,1] 224x1o,224x1o', 'w[2,2] 224x2e,224x2e'])\n",
      "dict_keys(['w[0,0] 224x0e,224x0e', 'w[1,1] 224x1o,224x1o', 'w[2,2] 224x2e,224x2e'])\n",
      "w[0,0] 224x0e,224x0e\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "w[1,1] 224x1o,224x1o\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "w[2,2] 224x2e,224x2e\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "\n",
      "linear_dn_0\n",
      "dict_keys(['w[0,0] 224x0e,224x0e', 'w[1,1] 224x1o,224x1o', 'w[2,2] 224x2e,224x2e', 'w[3,3] 224x3o,224x3o'])\n",
      "dict_keys(['w[0,0] 224x0e,224x0e', 'w[1,1] 224x1o,224x1o', 'w[2,2] 224x2e,224x2e', 'w[3,3] 224x3o,224x3o'])\n",
      "w[0,0] 224x0e,224x0e\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "w[1,1] 224x1o,224x1o\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "w[2,2] 224x2e,224x2e\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "w[3,3] 224x3o,224x3o\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "\n",
      "linear_dn_1\n",
      "dict_keys(['w[0,0] 672x0e,224x0e', 'w[1,1] 1120x1o,224x1o', 'w[2,2] 1120x2e,224x2e', 'w[3,3] 896x3o,224x3o'])\n",
      "dict_keys(['w[0,0] 672x0e,224x0e', 'w[1,1] 1120x1o,224x1o', 'w[2,2] 1120x2e,224x2e', 'w[3,3] 896x3o,224x3o'])\n",
      "w[0,0] 672x0e,224x0e\n",
      "(672, 224)\n",
      "(672, 224)\n",
      "w[1,1] 1120x1o,224x1o\n",
      "(1120, 224)\n",
      "(1120, 224)\n",
      "w[2,2] 1120x2e,224x2e\n",
      "(1120, 224)\n",
      "(1120, 224)\n",
      "w[3,3] 896x3o,224x3o\n",
      "(896, 224)\n",
      "(896, 224)\n",
      "\n",
      "linear_contraction_0\n",
      "dict_keys(['w[0,0] 224x0e,224x0e', 'w[1,1] 224x1o,224x1o', 'w[2,2] 224x2e,224x2e'])\n",
      "dict_keys(['w[0,0] 224x0e,224x0e', 'w[1,1] 224x1o,224x1o', 'w[2,2] 224x2e,224x2e'])\n",
      "w[0,0] 224x0e,224x0e\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "w[1,1] 224x1o,224x1o\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "w[2,2] 224x2e,224x2e\n",
      "(224, 224)\n",
      "(224, 224)\n",
      "\n",
      "linear_contraction_1\n",
      "dict_keys(['w[0,0] 224x0e,224x0e'])\n",
      "dict_keys(['w[0,0] 224x0e,224x0e'])\n",
      "w[0,0] 224x0e,224x0e\n",
      "(224, 224)\n",
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "# linear up\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print(f\"linear_up_{i}\")\n",
    "    up_params_torch = linear_torch_to_jax(torch_model.interactions[i].linear_up)\n",
    "    print(up_params_torch.keys())\n",
    "    print(nn_energy_params[f\"linear_up_{i}\"].keys())\n",
    "    for k in up_params_torch.keys():\n",
    "        print(k)\n",
    "        print(up_params_torch[k].shape)\n",
    "        print(nn_energy_params[f\"linear_up_{i}\"][k].shape)\n",
    "        assert up_params_torch[k].shape == nn_energy_params[f\"linear_up_{i}\"][k].shape\n",
    "        nn_energy_params[f\"linear_up_{i}\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "# linear down\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print(f\"linear_dn_{i}\")\n",
    "    up_params_torch = linear_torch_to_jax(torch_model.interactions[i].linear)\n",
    "    print(up_params_torch.keys())\n",
    "    print(nn_energy_params[f\"linear_dn_{i}\"].keys())\n",
    "    for k in up_params_torch.keys():\n",
    "        print(k)\n",
    "        print(up_params_torch[k].shape)\n",
    "        print(nn_energy_params[f\"linear_dn_{i}\"][k].shape)\n",
    "        assert up_params_torch[k].shape == nn_energy_params[f\"linear_dn_{i}\"][k].shape\n",
    "        nn_energy_params[f\"linear_dn_{i}\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "# contraction linear\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print(f\"linear_contraction_{i}\")\n",
    "    up_params_torch = linear_torch_to_jax(torch_model.products[i].linear)\n",
    "    print(up_params_torch.keys())\n",
    "    print(nn_energy_params[f\"linear_contraction_{i}\"].keys())\n",
    "    for k in up_params_torch.keys():\n",
    "        print(k)\n",
    "        print(up_params_torch[k].shape)\n",
    "        try:\n",
    "            print(nn_energy_params[f\"linear_contraction_{i}\"][k].shape)\n",
    "            assert up_params_torch[k].shape == nn_energy_params[f\"linear_contraction_{i}\"][k].shape\n",
    "            nn_energy_params[f\"linear_contraction_{i}\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "        except KeyError:\n",
    "            print(f\"key {k} not found in nn_energy_params[f'linear_contraction_{i}']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_readout_0\n",
      "dict_keys(['w[0,0] 224x0e,1x0e'])\n",
      "dict_keys(['w[0,0] 224x0e,1x0e'])\n",
      "w[0,0] 224x0e,1x0e\n",
      "(224, 1)\n",
      "(224, 1)\n",
      "\n",
      "hidden_linear_readout_last\n",
      "dict_keys(['w[0,0] 224x0e,16x0e'])\n",
      "dict_keys(['w[0,0] 224x0e,16x0e'])\n",
      "w[0,0] 224x0e,16x0e\n",
      "(224, 16)\n",
      "(224, 16)\n",
      "silu_norm 1.6765620560798837 1.6791767923989418\n",
      "\n",
      "linear_readout_last\n",
      "dict_keys(['w[0,0] 16x0e,1x0e'])\n",
      "dict_keys(['w[0,0] 16x0e,1x0e'])\n",
      "w[0,0] 16x0e,1x0e\n",
      "(16, 1)\n",
      "(16, 1)\n"
     ]
    }
   ],
   "source": [
    "# readout block\n",
    "print(f\"linear_readout_0\")\n",
    "up_params_torch = linear_torch_to_jax(torch_model.readouts[0].linear)\n",
    "print(up_params_torch.keys())\n",
    "print(nn_energy_params[f\"linear_readout_0\"].keys())\n",
    "for k in up_params_torch.keys():\n",
    "    print(k)\n",
    "    print(up_params_torch[k].shape)\n",
    "    print(nn_energy_params[f\"linear_readout_0\"][k].shape)\n",
    "    assert up_params_torch[k].shape == nn_energy_params[f\"linear_readout_0\"][k].shape\n",
    "    nn_energy_params[f\"linear_readout_0\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "print()\n",
    "print(f\"hidden_linear_readout_last\")\n",
    "up_params_torch = linear_torch_to_jax(torch_model.readouts[1].linear_1)\n",
    "print(up_params_torch.keys())\n",
    "print(nn_energy_params[f\"hidden_linear_readout_last\"].keys())\n",
    "for k in up_params_torch.keys():\n",
    "    print(k)\n",
    "    print(up_params_torch[k].shape)\n",
    "    print(nn_energy_params[f\"hidden_linear_readout_last\"][k].shape)\n",
    "    assert up_params_torch[k].shape == nn_energy_params[f\"hidden_linear_readout_last\"][k].shape\n",
    "    nn_energy_params[f\"hidden_linear_readout_last\"][k] = jnp.array(up_params_torch[k],dtype=jnp.float64)\n",
    "\n",
    "\n",
    "f,silu_norm = normalize_activation(jax.nn.silu,return_scale=True)\n",
    "print(\"silu_norm\",silu_norm,torch_model.readouts[1].non_linearity.acts[0].cst)\n",
    "cst = torch_model.readouts[1].non_linearity.acts[0].cst / silu_norm\n",
    "\n",
    "print()\n",
    "print(f\"linear_readout_last\")\n",
    "up_params_torch = linear_torch_to_jax(torch_model.readouts[1].linear_2)\n",
    "print(up_params_torch.keys())\n",
    "print(nn_energy_params[f\"linear_readout_last\"].keys())\n",
    "for k in up_params_torch.keys():\n",
    "    print(k)\n",
    "    print(up_params_torch[k].shape)\n",
    "    print(nn_energy_params[f\"linear_readout_last\"][k].shape)\n",
    "    assert up_params_torch[k].shape == nn_energy_params[f\"linear_readout_last\"][k].shape\n",
    "    nn_energy_params[f\"linear_readout_last\"][k] = cst*jnp.array(up_params_torch[k],dtype=jnp.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radial net 0\n",
      "dict_keys(['Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])\n",
      "(8, 64) (8, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 896) (64, 896)\n",
      "\n",
      "radial net 1\n",
      "dict_keys(['Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])\n",
      "(8, 64) (8, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 64) (64, 64)\n",
      "cst 1.001559582187594\n",
      "(64, 3808) (64, 3808)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# radial net\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"radial net\",i)\n",
    "    print(nn_energy_params[f\"radial_network_{i}\"].keys())\n",
    "    w  = torch_model.interactions[i].conv_tp_weights.layer0.weight.detach().numpy()\n",
    "    w = w/w.shape[0]**0.5\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_0\"][\"kernel\"].shape)\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_0\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_0\"][\"kernel\"] = jnp.array(w,dtype=jnp.float64)\n",
    "\n",
    "    cst = torch_model.interactions[i].conv_tp_weights.layer0.act.cst/ silu_norm\n",
    "    print(\"cst\",cst)\n",
    "\n",
    "    w  = cst*torch_model.interactions[i].conv_tp_weights.layer1.weight.detach().numpy()\n",
    "    w = w/w.shape[0]**0.5\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_1\"][\"kernel\"].shape)\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_1\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_1\"][\"kernel\"] =  jnp.array(w,dtype=jnp.float64)\n",
    "\n",
    "    cst = torch_model.interactions[i].conv_tp_weights.layer0.act.cst/ silu_norm\n",
    "    print(\"cst\",cst)\n",
    "\n",
    "    w  = cst*torch_model.interactions[i].conv_tp_weights.layer2.weight.detach().numpy()\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_2\"][\"kernel\"].shape)\n",
    "    w = w/w.shape[0]**0.5\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_2\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_2\"][\"kernel\"] =  jnp.array(w,dtype=jnp.float64)\n",
    "\n",
    "    cst = torch_model.interactions[i].conv_tp_weights.layer0.act.cst/ silu_norm\n",
    "    print(\"cst\",cst)\n",
    "\n",
    "    w  = cst*torch_model.interactions[i].conv_tp_weights.layer3.weight.detach().numpy()\n",
    "    w = w/w.shape[0]**0.5\n",
    "    print(w.shape,nn_energy_params[f\"radial_network_{i}\"][\"Dense_3\"][\"kernel\"].shape)\n",
    "    assert w.shape == nn_energy_params[f\"radial_network_{i}\"][\"Dense_3\"][\"kernel\"].shape\n",
    "    nn_energy_params[f\"radial_network_{i}\"][\"Dense_3\"][\"kernel\"] =  jnp.array(w,dtype=jnp.float64)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mace.tools.cg import U_matrix_real\n",
    "from e3nn_jax import reduced_tensor_product_basis\n",
    "from e3nn import o3\n",
    "import e3nn_jax\n",
    "\n",
    "def get_reordering(order,ir_out):\n",
    "    # torch \n",
    "    irreps_in = o3.Irreps(\"1x0e + 1x1o + 1x2e + 1x3o\")\n",
    "    irrep_out = o3.Irreps(\"1x\"+ir_out)\n",
    "    U_matrix = U_matrix_real(irreps_in=irreps_in, irreps_out=irrep_out, correlation=order)[-1].numpy()\n",
    "    U_matrix_abs = np.abs(U_matrix)\n",
    "\n",
    "    # jax\n",
    "    irreps_in = e3nn_jax.Irreps(\"1x0e + 1x1o + 1x2e + 1x3o\")\n",
    "    irrep_out = e3nn_jax.Irrep(ir_out)\n",
    "    U = reduced_tensor_product_basis([irreps_in] * order, keep_ir=[irrep_out]).list[-1]\n",
    "    U = np.moveaxis(U,-1,0)\n",
    "    U_abs = np.abs(U)\n",
    "\n",
    "    index = []\n",
    "    signs = []\n",
    "    for i in range(U.shape[-1]):\n",
    "        diffs = []\n",
    "        for k in range(U_matrix.shape[-1]):\n",
    "            diff = np.max(np.abs(U_abs[...,i]-U_matrix_abs[...,k]))\n",
    "            diffs.append(diff)\n",
    "        k = np.argmin(diffs)\n",
    "        diff = diffs[k]\n",
    "        if diff>1.e-6:\n",
    "            print(i,k,diff)\n",
    "            raise ValueError(\"diff too large\")\n",
    "        \n",
    "        index.append(k)\n",
    "        diff = np.max(np.abs(U[...,i]-U_matrix[...,k]))\n",
    "        if diff < 1.e-6:\n",
    "            signs.append(1.)\n",
    "        else:\n",
    "            diff = np.max(np.abs(U[...,i]+U_matrix[...,k]))\n",
    "            if diff < 1.e-6:\n",
    "                print(i,k,\"global sign differ\")\n",
    "                signs.append(-1.)\n",
    "            else:\n",
    "                raise ValueError(\"local sign differ\")\n",
    "    print(order,ir_out,len(index),U.shape[-1],len(signs))\n",
    "    return np.array(index),np.array(signs)\n",
    "    return np.arange(U.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layer 0 -> ['w1_0e', 'w1_1o', 'w1_2e', 'w2_0e', 'w2_1o', 'w2_2e', 'w3_0e', 'w3_1o', 'w3_2e']\n",
      "   ir 0e l 0 corr 3\n",
      "     w3_0e (10, 23, 224) (120, 23, 224)\n",
      "3 0e 23 23 23\n",
      "1.455638554091279\n",
      "     w2_0e (10, 4, 224) (120, 4, 224)\n",
      "2 0e 4 4 4\n",
      "1.3833557012728157\n",
      "     w1_0e (10, 1, 224) (120, 1, 224)\n",
      "1 0e 1 1 1\n",
      "0.5838673742783771\n",
      "   ir 1o l 1 corr 3\n",
      "     w3_1o (10, 51, 224) (120, 51, 224)\n",
      "17 17 global sign differ\n",
      "18 18 global sign differ\n",
      "35 21 global sign differ\n",
      "36 22 global sign differ\n",
      "44 44 global sign differ\n",
      "46 46 global sign differ\n",
      "3 1o 51 51 51\n",
      "1.3892637638000955\n",
      "     w2_1o (10, 6, 224) (120, 6, 224)\n",
      "2 1o 6 6 6\n",
      "1.6495189089561173\n",
      "     w1_1o (10, 1, 224) (120, 1, 224)\n",
      "1 1o 1 1 1\n",
      "0.614487849317727\n",
      "   ir 2e l 2 corr 3\n",
      "     w3_2e (10, 65, 224) (120, 65, 224)\n",
      "16 16 global sign differ\n",
      "35 23 global sign differ\n",
      "36 25 global sign differ\n",
      "37 20 global sign differ\n",
      "38 22 global sign differ\n",
      "53 53 global sign differ\n",
      "55 55 global sign differ\n",
      "3 2e 65 65 65\n",
      "1.3573949041483202\n",
      "     w2_2e (10, 7, 224) (120, 7, 224)\n",
      "2 2e 7 7 7\n",
      "1.5262353521071717\n",
      "     w1_2e (10, 1, 224) (120, 1, 224)\n",
      "1 2e 1 1 1\n",
      "0.6746767091225503\n",
      "\n",
      "layer 1 -> ['w1_0e', 'w2_0e', 'w3_0e']\n",
      "   ir 0e l 0 corr 3\n",
      "     w3_0e (10, 23, 224) (120, 23, 224)\n",
      "3 0e 23 23 23\n",
      "1.971209800279653\n",
      "     w2_0e (10, 4, 224) (120, 4, 224)\n",
      "2 0e 4 4 4\n",
      "1.9115796625753665\n",
      "     w1_0e (10, 1, 224) (120, 1, 224)\n",
      "1 0e 1 1 1\n",
      "1.0600793217210112\n"
     ]
    }
   ],
   "source": [
    "ir_layers = [[\"0e\",\"1o\",\"2e\"],[\"0e\"]]\n",
    "for layer in [0,1]:\n",
    "    print()\n",
    "    print(\"layer\",layer,\"->\",list(nn_energy_params[f\"SymmetricContraction_{layer}\"].keys()))\n",
    "\n",
    "    for ir in ir_layers[layer]:\n",
    "        l=int(ir[0])\n",
    "        corr = len(torch_model.products[layer].symmetric_contractions.contractions[l].weights)+1\n",
    "        print(\"   ir\",ir,\"l\",l,\"corr\",corr)\n",
    "\n",
    "        w=torch_model.products[layer].symmetric_contractions.contractions[l].weights_max.detach().cpu().numpy()\n",
    "\n",
    "        print(f\"     w{corr}_{ir}\",w.shape,nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"].shape) \n",
    "        assert w.shape[1:] == nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"].shape[1:]\n",
    "\n",
    "        # reorder weights\n",
    "        index,signs = get_reordering(corr,ir)\n",
    "        w = w[:,index,:]*signs[None,:,None]\n",
    "\n",
    "\n",
    "        # last order\n",
    "        skip_params = np.zeros_like(nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"])\n",
    "        for i,z in enumerate(species_order):\n",
    "            skip_params[z,:,:] = w[i,:,:]\n",
    "        nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{corr}_{ir}\"] = jnp.asarray(skip_params,dtype=jnp.float64)\n",
    "\n",
    "        print(np.abs(w).mean())\n",
    "\n",
    "        for c in range(corr-1):\n",
    "            order = corr-c-1\n",
    "            w=torch_model.products[layer].symmetric_contractions.contractions[l].weights[c].detach().cpu().numpy()\n",
    "            print(f\"     w{order}_{ir}\",w.shape,nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"].shape)\n",
    "            assert w.shape[1:] == nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"].shape[1:]\n",
    "\n",
    "            index,signs = get_reordering(order,ir)\n",
    "            w = w[:,index,:]*signs[None,:,None]\n",
    "\n",
    "            skip_params = np.zeros_like(nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"])\n",
    "            for i,z in enumerate(species_order):\n",
    "                skip_params[z,:,:] = w[i,:,:]\n",
    "            nn_energy_params[f\"SymmetricContraction_{layer}\"][f\"w{order}_{ir}\"] = jnp.asarray(skip_params,dtype=jnp.float64)\n",
    "            print(np.abs(w).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACE torch -2080.921473483096\n"
     ]
    }
   ],
   "source": [
    "atoms.calc = calc  \n",
    "print(\"MACE torch\",atoms.get_potential_energy())\n",
    "\n",
    "variables[\"params\"][\"nn_energy\"] = nn_energy_params  \n",
    "model_fennol.variables = variables \n",
    "\n",
    "model_fennol.save(model_name+\".fnx\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACE fennol -2080.921473483096\n"
     ]
    }
   ],
   "source": [
    "model_fennol_load = FENNIX.load(model_name+\".fnx\")\n",
    "# print(\"##############################\")\n",
    "# print(\"Restored model\")\n",
    "# for k in model_fennol_load.variables[\"params\"][\"nn_energy\"].keys():\n",
    "#     print(k)\n",
    "#     if isinstance(nn_energy_params[k],dict):\n",
    "#         for kk in nn_energy_params[k].keys():\n",
    "#             print(\"   \",kk)\n",
    "calc_fennol = FENNIXCalculator(model=model_fennol_load,atoms=atoms,use_float64=True)\n",
    "atoms.calc = calc_fennol\n",
    "print(\"MACE fennol\",atoms.get_potential_energy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['batch_index', 'coordinates', 'energy_shift', 'graph', 'natoms', 'nn_energy', 'nn_energy_node_feats', 'species'])\n",
      "[-4.11289955 -2.86547566 -2.86547566]\n"
     ]
    }
   ],
   "source": [
    "species = atoms.get_atomic_numbers()\n",
    "coordinates = atoms.get_positions()\n",
    "output = model_fennol_load(coordinates=coordinates,species=species)\n",
    "print(output.keys())\n",
    "print(output[\"nn_energy\"]*au.EV)\n",
    "# print(output[\"nn_energy_node_feats\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.11289955 -2.86547566 -2.86547566]\n"
     ]
    }
   ],
   "source": [
    "node_feats_torch = calc.get_descriptors(atoms)\n",
    "# print(node_feats_torch[0])\n",
    "print(calc.get_property(\"node_energy\",atoms=atoms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
